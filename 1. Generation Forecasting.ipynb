{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8759e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score # New\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.preprocessing\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, Multiply, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "# regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D\n",
    "from scipy import signal\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import svd\n",
    "import gc\n",
    "from keijzer import *\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate, Dropout, Activation, Multiply, Embedding, Layer, Reshape\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D  \n",
    "from keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "#from keras.ops import convert_to_tensor, convert_to_numpy\n",
    "#from keras.utils import plot_model\n",
    "from keras import activations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class LossHistory(tensorflow.keras.callbacks.Callback):  # history = LossHistory()\n",
    "    def init(self):  # history.init()\n",
    "        self.losses = []\n",
    "        # self.accs = []\n",
    "        self.val_losses = []\n",
    "        # self.val_accs = []\n",
    "        self.rmses = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = []\n",
    "        self.val_rmses = []\n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        # self.val_accs.append(logs.get('val_accuracy\n",
    "        self.rmses.append(logs.get('root_mean_squared_error'))\n",
    "        self.mses.append(logs.get('mean_squared_error'))\n",
    "        self.maes.append(logs.get('mean_absolute_error'))\n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error'))\n",
    "        self.val_rmses.append(logs.get('val_root_mean_squared_error'))\n",
    "        self.val_mses.append(logs.get('val_mean_squared_error'))\n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error'))\n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))\n",
    "\n",
    "\n",
    "\n",
    "def root_squared_mean_error(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true-K.mean(y_true)))*100\n",
    "\n",
    "def mse_mae(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))+K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10384b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # 여러개 사용시 '0,1,2' 식으로 하나의 문자열에 입력\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') # 호스트 러나임에 표시되는 GPU 장치 목록 반환\n",
    "\n",
    "if gpus: # 반환된 GPU 장치 목록이 있다면\n",
    "    try: # 해당 장치에 대한 메모리 증가 활성화 여부 설정\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e: # try문 실패시에 에러문구 출력\n",
    "        print(e)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdbb5b",
   "metadata": {},
   "source": [
    "## Import Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cede92a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time-Date stamp</th>\n",
       "      <th>Date</th>\n",
       "      <th>ERCOT Load</th>\n",
       "      <th>Total Wind Output</th>\n",
       "      <th>Total Wind Installed</th>\n",
       "      <th>Wind Output, % of Load</th>\n",
       "      <th>Wind Output, % of Installed</th>\n",
       "      <th>1-hr MW change</th>\n",
       "      <th>1-hr % change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2020 00:00:00</td>\n",
       "      <td>Jan-01</td>\n",
       "      <td>36823</td>\n",
       "      <td>12335</td>\n",
       "      <td>27040</td>\n",
       "      <td>33.5</td>\n",
       "      <td>45.617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-01-2020 01:00:00</td>\n",
       "      <td>Jan-01</td>\n",
       "      <td>36511</td>\n",
       "      <td>13300</td>\n",
       "      <td>27040</td>\n",
       "      <td>36.4</td>\n",
       "      <td>49.187</td>\n",
       "      <td>965.4</td>\n",
       "      <td>7.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-01-2020 02:00:00</td>\n",
       "      <td>Jan-01</td>\n",
       "      <td>36022</td>\n",
       "      <td>13586</td>\n",
       "      <td>27040</td>\n",
       "      <td>37.7</td>\n",
       "      <td>50.245</td>\n",
       "      <td>286.2</td>\n",
       "      <td>2.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-01-2020 03:00:00</td>\n",
       "      <td>Jan-01</td>\n",
       "      <td>35790</td>\n",
       "      <td>14107</td>\n",
       "      <td>27040</td>\n",
       "      <td>39.4</td>\n",
       "      <td>52.171</td>\n",
       "      <td>520.9</td>\n",
       "      <td>3.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-01-2020 04:00:00</td>\n",
       "      <td>Jan-01</td>\n",
       "      <td>35642</td>\n",
       "      <td>14887</td>\n",
       "      <td>27040</td>\n",
       "      <td>41.8</td>\n",
       "      <td>55.055</td>\n",
       "      <td>779.6</td>\n",
       "      <td>5.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>12-31-2023 20:00:00</td>\n",
       "      <td>Dec-31</td>\n",
       "      <td>44887</td>\n",
       "      <td>17161</td>\n",
       "      <td>38631</td>\n",
       "      <td>38.23</td>\n",
       "      <td>44.42</td>\n",
       "      <td>2858</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>12-31-2023 21:00:00</td>\n",
       "      <td>Dec-31</td>\n",
       "      <td>44013</td>\n",
       "      <td>18301</td>\n",
       "      <td>38631</td>\n",
       "      <td>41.58</td>\n",
       "      <td>47.37</td>\n",
       "      <td>1139</td>\n",
       "      <td>6.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>12-31-2023 22:00:00</td>\n",
       "      <td>Dec-31</td>\n",
       "      <td>43280</td>\n",
       "      <td>18607</td>\n",
       "      <td>38631</td>\n",
       "      <td>42.99</td>\n",
       "      <td>48.17</td>\n",
       "      <td>307</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>12-31-2023 23:00:00</td>\n",
       "      <td>Dec-31</td>\n",
       "      <td>42319</td>\n",
       "      <td>17866</td>\n",
       "      <td>38631</td>\n",
       "      <td>42.22</td>\n",
       "      <td>46.25</td>\n",
       "      <td>-741</td>\n",
       "      <td>-3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35064</th>\n",
       "      <td>01-01-2024 00:00:00</td>\n",
       "      <td>Jan-01</td>\n",
       "      <td>41682</td>\n",
       "      <td>15869</td>\n",
       "      <td>38631</td>\n",
       "      <td>38.07</td>\n",
       "      <td>41.08</td>\n",
       "      <td>-1996</td>\n",
       "      <td>-11.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35065 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time-Date stamp    Date ERCOT Load Total Wind Output  \\\n",
       "0      01-01-2020 00:00:00  Jan-01      36823             12335   \n",
       "1      01-01-2020 01:00:00  Jan-01      36511             13300   \n",
       "2      01-01-2020 02:00:00  Jan-01      36022             13586   \n",
       "3      01-01-2020 03:00:00  Jan-01      35790             14107   \n",
       "4      01-01-2020 04:00:00  Jan-01      35642             14887   \n",
       "...                    ...     ...        ...               ...   \n",
       "35060  12-31-2023 20:00:00  Dec-31      44887             17161   \n",
       "35061  12-31-2023 21:00:00  Dec-31      44013             18301   \n",
       "35062  12-31-2023 22:00:00  Dec-31      43280             18607   \n",
       "35063  12-31-2023 23:00:00  Dec-31      42319             17866   \n",
       "35064  01-01-2024 00:00:00  Jan-01      41682             15869   \n",
       "\n",
       "      Total Wind Installed Wind Output, % of Load Wind Output, % of Installed  \\\n",
       "0                    27040                   33.5                      45.617   \n",
       "1                    27040                   36.4                      49.187   \n",
       "2                    27040                   37.7                      50.245   \n",
       "3                    27040                   39.4                      52.171   \n",
       "4                    27040                   41.8                      55.055   \n",
       "...                    ...                    ...                         ...   \n",
       "35060                38631                  38.23                       44.42   \n",
       "35061                38631                  41.58                       47.37   \n",
       "35062                38631                  42.99                       48.17   \n",
       "35063                38631                  42.22                       46.25   \n",
       "35064                38631                  38.07                       41.08   \n",
       "\n",
       "      1-hr MW change 1-hr % change  \n",
       "0                NaN           NaN  \n",
       "1              965.4         7.826  \n",
       "2              286.2         2.152  \n",
       "3              520.9         3.834  \n",
       "4              779.6         5.527  \n",
       "...              ...           ...  \n",
       "35060           2858         19.99  \n",
       "35061           1139          6.64  \n",
       "35062            307          1.67  \n",
       "35063           -741         -3.98  \n",
       "35064          -1996        -11.17  \n",
       "\n",
       "[35065 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2020.csv'\n",
    "data_source1 = pd.read_csv(data_path)\n",
    "dat_source1 = pd.DataFrame(data_source1)\n",
    "#dat_source1 = dat_source1.iloc[:-3,:]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2021.csv'\n",
    "data_source2 = pd.read_csv(data_path)\n",
    "dat_source2 = pd.DataFrame(data_source2)\n",
    "dat_source2 = dat_source2.iloc[:8761,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2022.csv'\n",
    "data_source3 = pd.read_csv(data_path)\n",
    "dat_source3 = pd.DataFrame(data_source3)\n",
    "dat_source3 = dat_source3.iloc[:8760,:9]\n",
    "\n",
    "data_path = 'C:/Users/smartgrid_AI/Desktop/windpower_ramprate/ercot_2023.csv'\n",
    "data_source4 = pd.read_csv(data_path)\n",
    "dat_source4 = pd.DataFrame(data_source4)\n",
    "dat_source4 = dat_source4.iloc[:8760,:9]\n",
    "\n",
    "dat_arr1 = np.array(dat_source1)\n",
    "dat_arr2 = np.array(dat_source2)\n",
    "dat_arr3 = np.array(dat_source3)\n",
    "dat_arr4 = np.array(dat_source4)\n",
    "\n",
    "dat_arr = np.concatenate([dat_arr1, dat_arr2, dat_arr3, dat_arr4],axis=0)\n",
    "dat_arr.shape # 8784, 8761, 8760, 8760\n",
    "\n",
    "dat_source = pd.DataFrame(dat_arr, columns=['Time-Date stamp','Date','ERCOT Load','Total Wind Output','Total Wind Installed','Wind Output, % of Load','Wind Output, % of Installed','1-hr MW change','1-hr % change'])\n",
    "dat_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76a8d8",
   "metadata": {},
   "source": [
    "### Dividing by Installed Capacity & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783787ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW % change</th>\n",
       "      <th>Total Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.56879</td>\n",
       "      <td>0.491864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.05769</td>\n",
       "      <td>0.502441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.92678</td>\n",
       "      <td>0.521709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.88462</td>\n",
       "      <td>0.550555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.580621</td>\n",
       "      <td>0.544749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>7.3982</td>\n",
       "      <td>0.444229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2.951</td>\n",
       "      <td>0.473739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>0.79211</td>\n",
       "      <td>0.48166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>-1.91815</td>\n",
       "      <td>0.462478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>-5.16942</td>\n",
       "      <td>0.410784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MW % change Total Wind Output\n",
       "0         3.56879          0.491864\n",
       "1         1.05769          0.502441\n",
       "2         1.92678          0.521709\n",
       "3         2.88462          0.550555\n",
       "4       -0.580621          0.544749\n",
       "...           ...               ...\n",
       "35059      7.3982          0.444229\n",
       "35060       2.951          0.473739\n",
       "35061     0.79211           0.48166\n",
       "35062    -1.91815          0.462478\n",
       "35063    -5.16942          0.410784\n",
       "\n",
       "[35064 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.DataFrame()\n",
    "dat['MW % change'] = np.array(dat_source.iloc[1:,3], dtype=float)-np.array(dat_source.iloc[0:-1,3], dtype=float)\n",
    "dat['MW % change'] = np.divide(np.array(dat['MW % change']),np.array(dat_source.iloc[1:,4]))*100\n",
    "dat['Total Wind Output'] = np.divide(np.array(dat_source.iloc[1:,3]), np.array(dat_source.iloc[1:,4]))\n",
    "\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b61f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>MW % change</th>\n",
       "      <th>Total Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>3.56879</td>\n",
       "      <td>0.491864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.05769</td>\n",
       "      <td>0.502441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.92678</td>\n",
       "      <td>0.521709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>2.88462</td>\n",
       "      <td>0.550555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>-0.580621</td>\n",
       "      <td>0.544749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>7.3982</td>\n",
       "      <td>0.444229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>2.951</td>\n",
       "      <td>0.473739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.79211</td>\n",
       "      <td>0.48166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>-1.91815</td>\n",
       "      <td>0.462478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2023</td>\n",
       "      <td>-5.16942</td>\n",
       "      <td>0.410784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year MW % change Total Wind Output\n",
       "0      2020     3.56879          0.491864\n",
       "1      2020     1.05769          0.502441\n",
       "2      2020     1.92678          0.521709\n",
       "3      2020     2.88462          0.550555\n",
       "4      2020   -0.580621          0.544749\n",
       "...     ...         ...               ...\n",
       "35059  2023      7.3982          0.444229\n",
       "35060  2023       2.951          0.473739\n",
       "35061  2023     0.79211           0.48166\n",
       "35062  2023    -1.91815          0.462478\n",
       "35063  2023    -5.16942          0.410784\n",
       "\n",
       "[35064 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ramp_ratio = 20\n",
    "load_ratio = 10\n",
    "year_arr = np.concatenate([np.ones(8784)*2020, np.ones(8760)*2021, np.ones(8760)*2022, np.ones(8760)*2023])\n",
    "year_df = pd.DataFrame(year_arr, columns=['Year'], dtype=int)\n",
    "wind_df = pd.concat([year_df, dat], axis=1)\n",
    "wind_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d288b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>MW % change</th>\n",
       "      <th>Total Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.495246</td>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35058</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.735512</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.620917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.631322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.454969</td>\n",
       "      <td>0.606127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  MW % change  Total Wind Output\n",
       "0      2020     0.620197           0.644724\n",
       "1      2020     0.544580           0.658617\n",
       "2      2020     0.570751           0.683924\n",
       "3      2020     0.599594           0.721813\n",
       "4      2020     0.495246           0.714187\n",
       "...     ...          ...                ...\n",
       "35058  2023     0.774643           0.484984\n",
       "35059  2023     0.735512           0.582157\n",
       "35060  2023     0.601593           0.620917\n",
       "35061  2023     0.536583           0.631322\n",
       "35062  2023     0.454969           0.606127\n",
       "\n",
       "[35063 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_df = wind_df.reset_index(drop=True)\n",
    "\n",
    "std_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_load = sklearn.preprocessing.MinMaxScaler()\n",
    "std_scaler_ramp = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "wind_df[['Total Wind Output']] = std_scaler.fit_transform(wind_df[['Total Wind Output']])\n",
    "wind_df[['MW % change']] = std_scaler_ramp.fit_transform(wind_df[['MW % change']])\n",
    "wind_df = wind_df.iloc[:-1,:]\n",
    "wind_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3004a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Wind Change</th>\n",
       "      <th>Wind Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.599594</td>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.495246</td>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35058</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.735512</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>0.620917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.536583</td>\n",
       "      <td>0.631322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.454969</td>\n",
       "      <td>0.606127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Wind Change  Wind Output\n",
       "0      2020     0.620197     0.644724\n",
       "1      2020     0.544580     0.658617\n",
       "2      2020     0.570751     0.683924\n",
       "3      2020     0.599594     0.721813\n",
       "4      2020     0.495246     0.714187\n",
       "...     ...          ...          ...\n",
       "35058  2023     0.774643     0.484984\n",
       "35059  2023     0.735512     0.582157\n",
       "35060  2023     0.601593     0.620917\n",
       "35061  2023     0.536583     0.631322\n",
       "35062  2023     0.454969     0.606127\n",
       "\n",
       "[35063 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ramp_df = pd.DataFrame(wind_df)\n",
    "ramp_df.columns=['Year', 'Wind Change', 'Wind Output']\n",
    "ramp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5593c3",
   "metadata": {},
   "source": [
    "## Wind Generation Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af041a7",
   "metadata": {},
   "source": [
    "### SVD-based Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3c315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, timesteps, output_timesteps):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timesteps -output_timesteps - 1):\n",
    "        a = dataset[i:(i + timesteps), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i + timesteps):(i+timesteps+output_timesteps), :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def svdadd(X, start, end, b_size):\n",
    "    dec_x = X[start:end, :, :]\n",
    "    U, S, Vh = svd(dec_x.reshape(b_size, -1), full_matrices=True)\n",
    "\n",
    "    high_sig = np.matmul(np.matmul(U[:,:high_ind], np.diag(S[0:high_ind])), Vh[:high_ind,:])\n",
    "    low_sig = X[:dec_num, :, :].reshape(b_size,-1)-high_sig\n",
    "\n",
    "    rec_x = np.zeros((b_size, b_size, high_ind))\n",
    "    \n",
    "    for i in range(high_ind):\n",
    "        rec_x[:,:,i] = np.matmul((U[:,i]*S[i]).reshape(-1,1), Vh[i,:].reshape(1,-1))    \n",
    "    return rec_x, low_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9da4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = np.array(wind_df.iloc[:,2]).reshape(-1,1)\n",
    "timesteps = 24*7\n",
    "output_timesteps = 24\n",
    "num_features = 1\n",
    "X, Y = create_dataset(norm_df, timesteps, output_timesteps)\n",
    "b_size = timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef7ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_num = timesteps\n",
    "high_ind = 35\n",
    "\n",
    "addX = np.zeros((X.shape[0], dec_num, high_ind+1))\n",
    "\n",
    "for k in range(X.shape[0]//dec_num):\n",
    "    rec_x, low_sig = svdadd(X, k*dec_num, (k+1)*dec_num, b_size)\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, :-1] = rec_x\n",
    "    addX[k*dec_num:k*dec_num+dec_num,:, -1] = low_sig\n",
    "    \n",
    "rec_x, low_sig = svdadd(X, X.shape[0]-dec_num, X.shape[0], b_size)\n",
    "addX[X.shape[0]-dec_num:X.shape[0], :, :-1] = rec_x\n",
    "addX[X.shape[0]-dec_num:X.shape[0],:, -1] = low_sig\n",
    "\n",
    "addX_det = addX[:, :, :-1]\n",
    "addX_det = np.sum(addX_det, axis=2)\n",
    "X = np.concatenate([X, addX], axis=2)\n",
    "num_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf18c7",
   "metadata": {},
   "source": [
    "### Training/Test Set Division & Shuffled Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d72b242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31383, 168, 37), (31383, 24), (3487, 168, 37), (3487, 24))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trvaX, teX, trvaY, teY = train_test_split(X, Y, test_size=0.1, shuffle=False)\n",
    "\n",
    "idx = np.arange(trvaX.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "strvaX = trvaX[idx]\n",
    "strvaY = trvaY[idx]\n",
    "strvaY = strvaY.reshape(-1, output_timesteps)\n",
    "\n",
    "trvaY = trvaY.reshape(-1, output_timesteps)\n",
    "teY = teY.reshape(-1, output_timesteps)\n",
    "\n",
    "trvaX.shape, trvaY.shape, teX.shape, teY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45619496",
   "metadata": {},
   "source": [
    "## Wind Generation Forecasting Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47eda8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAEMS(y_true, y_pred):\n",
    "    return K.mean((K.abs(y_pred - y_true))*K.square(y_true))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1a1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9a7194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 168, 37)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 37, 168)      0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 37, 168)      28392       permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 37, 168)      28392       permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 37, 168)      0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 168, 37)      0           multiply_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 168, 37)      0           input_3[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 168, 256)     9728        multiply_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 168, 256)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 168, 256)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 168, 256)     0           activation_20[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 168, 37)      18981       multiply_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 168, 37)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 168, 37)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 168, 37)      0           activation_22[0][0]              \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 168, 37)      0           input_3[0][0]                    \n",
      "                                                                 multiply_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 168, 37)      0           input_3[0][0]                    \n",
      "                                                                 multiply_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 168, 256)     9728        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 168, 256)     9728        subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 168, 256)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 168, 256)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 168, 256)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 168, 256)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 168, 256)     0           activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 168, 256)     0           activation_28[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 168, 37)      18981       multiply_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 168, 37)      18981       multiply_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 168, 37)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 168, 37)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 168, 37)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 168, 37)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 168, 37)      0           activation_26[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 168, 37)      0           activation_30[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 168, 37)      0           add_4[0][0]                      \n",
      "                                                                 multiply_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 168, 37)      0           subtract_2[0][0]                 \n",
      "                                                                 multiply_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 168, 148)     0           add_5[0][0]                      \n",
      "                                                                 subtract_3[0][0]                 \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 168, 256)     38144       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 168, 256)     38144       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 168, 256)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 168, 256)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 168, 256)     0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 168, 256)     0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, 168, 256)     0           activation_32[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_24 (Multiply)          (None, 168, 256)     0           activation_36[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 168, 37)      18981       multiply_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 168, 37)      18981       multiply_24[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 168, 37)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 168, 37)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 168, 37)      0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 168, 37)      0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, 168, 37)      0           activation_34[0][0]              \n",
      "                                                                 activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_25 (Multiply)          (None, 168, 37)      0           activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 168, 37)      0           add_5[0][0]                      \n",
      "                                                                 multiply_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 168, 37)      0           subtract_3[0][0]                 \n",
      "                                                                 multiply_25[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 168, 222)     0           add_6[0][0]                      \n",
      "                                                                 subtract_4[0][0]                 \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 168, 256)     57088       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 168, 256)     57088       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 168, 256)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 168, 256)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 168, 256)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 168, 256)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_26 (Multiply)          (None, 168, 256)     0           activation_40[0][0]              \n",
      "                                                                 activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_28 (Multiply)          (None, 168, 256)     0           activation_44[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 168, 37)      18981       multiply_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 168, 37)      18981       multiply_28[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 168, 37)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 168, 37)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 168, 37)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 168, 37)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_27 (Multiply)          (None, 168, 37)      0           activation_42[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_29 (Multiply)          (None, 168, 37)      0           activation_46[0][0]              \n",
      "                                                                 activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 168, 37)      0           add_6[0][0]                      \n",
      "                                                                 multiply_27[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 168, 37)      0           subtract_3[0][0]                 \n",
      "                                                                 multiply_29[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 168, 296)     0           add_7[0][0]                      \n",
      "                                                                 subtract_5[0][0]                 \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 168, 256)     76032       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 168, 256)     76032       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 168, 256)     0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 168, 256)     0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 168, 256)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 168, 256)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_30 (Multiply)          (None, 168, 256)     0           activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_32 (Multiply)          (None, 168, 256)     0           activation_52[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 168, 37)      18981       multiply_30[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 168, 37)      18981       multiply_32[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 168, 37)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 168, 37)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 168, 37)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 168, 37)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_31 (Multiply)          (None, 168, 37)      0           activation_50[0][0]              \n",
      "                                                                 activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_33 (Multiply)          (None, 168, 37)      0           activation_54[0][0]              \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 168, 37)      0           add_7[0][0]                      \n",
      "                                                                 multiply_31[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_6 (Subtract)           (None, 168, 37)      0           subtract_5[0][0]                 \n",
      "                                                                 multiply_33[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 168, 370)     0           add_8[0][0]                      \n",
      "                                                                 subtract_6[0][0]                 \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 168, 74)      0           add_8[0][0]                      \n",
      "                                                                 subtract_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 168, 1110)    0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 168, 720)     920880      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 168, 720)     0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 168, 360)     320040      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 168, 360)     0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 360)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 24)           8664        global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,848,909\n",
      "Trainable params: 1,848,909\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_features = trvaX.shape[2]\n",
    "output_timesteps = trvaY.shape[1]\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    alpha=1.0\n",
    "    gamma=1.2\n",
    "    beta = 1\n",
    "    hfilters = 256\n",
    "    hkernel_size1 = 1\n",
    "    hkernel_size2 = 2\n",
    "    \n",
    "    visible1 = Input(shape=(timesteps, num_features))\n",
    "   \n",
    "    ## Attention Block\n",
    "    per1 = Permute((2,1))(visible1)\n",
    "    den1a = Dense(timesteps, activation='tanh')(per1)\n",
    "    den1b = Dense(timesteps, activation='sigmoid')(per1)\n",
    "    den1 = Multiply()([den1a, den1b])\n",
    "    per2 = Permute((2,1), name='attention_vec')(den1)\n",
    "    mul1 = Multiply()([visible1, per2])\n",
    "    \n",
    "    ## Series PN DCCNN Blocks 1 ~ 6\n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=1)(mul1)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d1)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    res01a = Add()([visible1, d1])   # (100, 25) (100, 25)\n",
    "    res01b = Subtract()([visible1, d1])\n",
    "\n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res01a)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    \n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    res02a = Add()([res01a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res01b) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res02b = Subtract()([res01b, d2])   # (100, 25) (100, 25) \n",
    "    res02 = Concatenate()([res02a, res02b, res01a, res01b])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res02)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=8)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    res03a = Add()([res02a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res02) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=8)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res03b = Subtract()([res02b, d2])   # (100, 25) (100, 25)\n",
    "    res03 = Concatenate()([res03a, res03b, res02])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=8)(res03)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    res04a = Add()([res03a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=8)(res03) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res04b = Subtract()([res02b, d2])   # (100, 25) (100, 25)\n",
    "    res04 = Concatenate()([res04a, res04b, res03])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res04)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    res05a = Add()([res04a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res04) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res05b = Subtract()([res04b, d2])   # (100, 25) (100, 25)\n",
    "    res05 = Concatenate()([res05a, res05b, res04])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res05)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=1)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "\n",
    "    res06a = Add()([res05a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res05) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=1)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "\n",
    "    res06b = Subtract()([res05b, d2])   # (100, 25) (100, 25)\n",
    "    res06 = Concatenate()([res05a, res05b])\n",
    "    \n",
    "    res10 = Concatenate()([res02, res03, res04, res05, res06])   # \n",
    "    \n",
    "    #print('res10 :', res10.shap)  # (None, 24, 11) \n",
    "    \n",
    "    ## Output Block\n",
    "    out = Conv1D(720, 1, padding='same', activation=PReLU())(res10)   # 256, 11X10=110\n",
    "    out = Dropout(0.2)(out)   #SpatialDropout1D\n",
    "    \n",
    "    out = Conv1D(360, 1, padding='same', activation=PReLU())(out) # 512,  110X5=550\n",
    "    out = Dropout(0.2)(out)\n",
    "    \n",
    "    out = GlobalAveragePooling1D()(out) # pool_size=2, strides=1\n",
    "    \n",
    "    out = Dense(24)(out) \n",
    "    model = Model(inputs=[visible1], outputs=[out])\n",
    "    \n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87488761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2821"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfdfb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss=MAEMS, optimizer='adam', metrics=['mse','mae', MAEMS])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=50)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "    #root_squared_mean_error\n",
    "    history = LossHistory()\n",
    "    history.init()\n",
    "    \n",
    "    #hist = model.fit(trX, trY, epochs=epochs, batch_size=batch_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0260b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  6/187 [..............................] - ETA: 20s - loss: 7.6836 - mse: 0.1467 - mae: 0.3121 - MAEMS: 7.6836WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0522s vs `on_train_batch_end` time: 0.0552s). Check your callbacks.\n",
      "187/187 [==============================] - 29s 132ms/step - loss: 3.4188 - mse: 0.0797 - mae: 0.2283 - MAEMS: 3.4180 - val_loss: 2.2062 - val_mse: 0.0530 - val_mae: 0.1913 - val_MAEMS: 2.2069\n",
      "Epoch 2/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 2.8273 - mse: 0.0536 - mae: 0.1835 - MAEMS: 2.8269 - val_loss: 1.9421 - val_mse: 0.0432 - val_mae: 0.1674 - val_MAEMS: 1.9433\n",
      "Epoch 3/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 2.5705 - mse: 0.0442 - mae: 0.1623 - MAEMS: 2.5703 - val_loss: 1.8692 - val_mse: 0.0394 - val_mae: 0.1541 - val_MAEMS: 1.8697\n",
      "Epoch 4/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 2.3780 - mse: 0.0387 - mae: 0.1488 - MAEMS: 2.3778 - val_loss: 1.8606 - val_mse: 0.0328 - val_mae: 0.1393 - val_MAEMS: 1.8613\n",
      "Epoch 5/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 2.2239 - mse: 0.0347 - mae: 0.1386 - MAEMS: 2.2238 - val_loss: 1.8340 - val_mse: 0.0341 - val_mae: 0.1412 - val_MAEMS: 1.8341\n",
      "Epoch 6/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 2.0787 - mse: 0.0312 - mae: 0.1301 - MAEMS: 2.0786 - val_loss: 1.8885 - val_mse: 0.0326 - val_mae: 0.1382 - val_MAEMS: 1.8877\n",
      "Epoch 7/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.9778 - mse: 0.0287 - mae: 0.1239 - MAEMS: 1.9777 - val_loss: 1.9343 - val_mse: 0.0272 - val_mae: 0.1268 - val_MAEMS: 1.9347\n",
      "Epoch 8/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.8641 - mse: 0.0265 - mae: 0.1180 - MAEMS: 1.8641 - val_loss: 1.8370 - val_mse: 0.0319 - val_mae: 0.1350 - val_MAEMS: 1.8376\n",
      "Epoch 9/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.7749 - mse: 0.0246 - mae: 0.1128 - MAEMS: 1.7748 - val_loss: 1.9650 - val_mse: 0.0271 - val_mae: 0.1265 - val_MAEMS: 1.9656\n",
      "Epoch 10/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.6376 - mse: 0.0224 - mae: 0.1062 - MAEMS: 1.6375 - val_loss: 1.8635 - val_mse: 0.0290 - val_mae: 0.1303 - val_MAEMS: 1.8642\n",
      "Epoch 11/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.6012 - mse: 0.0213 - mae: 0.1033 - MAEMS: 1.6012 - val_loss: 1.9395 - val_mse: 0.0265 - val_mae: 0.1249 - val_MAEMS: 1.9408\n",
      "Epoch 12/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.5509 - mse: 0.0202 - mae: 0.1002 - MAEMS: 1.5508 - val_loss: 2.0733 - val_mse: 0.0240 - val_mae: 0.1204 - val_MAEMS: 2.0741\n",
      "Epoch 13/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.4507 - mse: 0.0186 - mae: 0.0951 - MAEMS: 1.4506 - val_loss: 2.0242 - val_mse: 0.0243 - val_mae: 0.1206 - val_MAEMS: 2.0250\n",
      "Epoch 14/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.3940 - mse: 0.0174 - mae: 0.0915 - MAEMS: 1.3939 - val_loss: 2.1815 - val_mse: 0.0226 - val_mae: 0.1175 - val_MAEMS: 2.1814\n",
      "Epoch 15/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.3792 - mse: 0.0168 - mae: 0.0898 - MAEMS: 1.3792 - val_loss: 1.8929 - val_mse: 0.0279 - val_mae: 0.1275 - val_MAEMS: 1.8937\n",
      "Epoch 16/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.3806 - mse: 0.0165 - mae: 0.0891 - MAEMS: 1.3805 - val_loss: 2.0452 - val_mse: 0.0245 - val_mae: 0.1216 - val_MAEMS: 2.0468\n",
      "Epoch 17/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.3355 - mse: 0.0158 - mae: 0.0867 - MAEMS: 1.3354 - val_loss: 1.9351 - val_mse: 0.0251 - val_mae: 0.1219 - val_MAEMS: 1.9371\n",
      "Epoch 18/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.2889 - mse: 0.0151 - mae: 0.0843 - MAEMS: 1.2889 - val_loss: 1.8852 - val_mse: 0.0264 - val_mae: 0.1251 - val_MAEMS: 1.8873\n",
      "Epoch 19/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.2544 - mse: 0.0144 - mae: 0.0821 - MAEMS: 1.2543 - val_loss: 1.9420 - val_mse: 0.0253 - val_mae: 0.1228 - val_MAEMS: 1.9436\n",
      "Epoch 20/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.2015 - mse: 0.0136 - mae: 0.0792 - MAEMS: 1.2015 - val_loss: 2.0461 - val_mse: 0.0231 - val_mae: 0.1186 - val_MAEMS: 2.0475\n",
      "Epoch 21/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.2049 - mse: 0.0133 - mae: 0.0786 - MAEMS: 1.2048 - val_loss: 2.1117 - val_mse: 0.0239 - val_mae: 0.1206 - val_MAEMS: 2.1111\n",
      "Epoch 22/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.1879 - mse: 0.0129 - mae: 0.0772 - MAEMS: 1.1878 - val_loss: 2.0589 - val_mse: 0.0233 - val_mae: 0.1191 - val_MAEMS: 2.0591\n",
      "Epoch 23/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.1797 - mse: 0.0127 - mae: 0.0765 - MAEMS: 1.1796 - val_loss: 1.9990 - val_mse: 0.0234 - val_mae: 0.1188 - val_MAEMS: 1.9986\n",
      "Epoch 24/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.1393 - mse: 0.0120 - mae: 0.0741 - MAEMS: 1.1392 - val_loss: 2.0289 - val_mse: 0.0242 - val_mae: 0.1210 - val_MAEMS: 2.0291\n",
      "Epoch 25/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.1187 - mse: 0.0116 - mae: 0.0728 - MAEMS: 1.1186 - val_loss: 1.9413 - val_mse: 0.0245 - val_mae: 0.1211 - val_MAEMS: 1.9409\n",
      "Epoch 26/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.0979 - mse: 0.0113 - mae: 0.0714 - MAEMS: 1.0978 - val_loss: 1.9916 - val_mse: 0.0236 - val_mae: 0.1195 - val_MAEMS: 1.9919\n",
      "Epoch 27/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.0847 - mse: 0.0110 - mae: 0.0704 - MAEMS: 1.0846 - val_loss: 1.9486 - val_mse: 0.0247 - val_mae: 0.1221 - val_MAEMS: 1.9501\n",
      "Epoch 28/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 1.0674 - mse: 0.0106 - mae: 0.0692 - MAEMS: 1.0674 - val_loss: 1.9857 - val_mse: 0.0242 - val_mae: 0.1212 - val_MAEMS: 1.9867\n",
      "Epoch 29/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 1.0509 - mse: 0.0103 - mae: 0.0678 - MAEMS: 1.0508 - val_loss: 2.0372 - val_mse: 0.0233 - val_mae: 0.1185 - val_MAEMS: 2.0364\n",
      "Epoch 30/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 1.0467 - mse: 0.0102 - mae: 0.0675 - MAEMS: 1.0466 - val_loss: 1.9682 - val_mse: 0.0249 - val_mae: 0.1225 - val_MAEMS: 1.9682\n",
      "Epoch 31/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.0309 - mse: 0.0099 - mae: 0.0665 - MAEMS: 1.0308 - val_loss: 2.0183 - val_mse: 0.0254 - val_mae: 0.1243 - val_MAEMS: 2.0177\n",
      "Epoch 32/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.0464 - mse: 0.0098 - mae: 0.0663 - MAEMS: 1.0464 - val_loss: 2.0069 - val_mse: 0.0244 - val_mae: 0.1210 - val_MAEMS: 2.0088\n",
      "Epoch 33/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.0449 - mse: 0.0097 - mae: 0.0661 - MAEMS: 1.0448 - val_loss: 1.9234 - val_mse: 0.0256 - val_mae: 0.1233 - val_MAEMS: 1.9254\n",
      "Epoch 34/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.0335 - mse: 0.0095 - mae: 0.0654 - MAEMS: 1.0336 - val_loss: 1.9726 - val_mse: 0.0244 - val_mae: 0.1215 - val_MAEMS: 1.9732\n",
      "Epoch 35/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 1.0460 - mse: 0.0095 - mae: 0.0657 - MAEMS: 1.0462 - val_loss: 2.1300 - val_mse: 0.0226 - val_mae: 0.1175 - val_MAEMS: 2.1309\n",
      "Epoch 36/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 1.0215 - mse: 0.0093 - mae: 0.0646 - MAEMS: 1.0214 - val_loss: 2.0914 - val_mse: 0.0233 - val_mae: 0.1186 - val_MAEMS: 2.0912\n",
      "Epoch 37/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 0.9699 - mse: 0.0087 - mae: 0.0621 - MAEMS: 0.9698 - val_loss: 2.0654 - val_mse: 0.0230 - val_mae: 0.1180 - val_MAEMS: 2.0669\n",
      "Epoch 38/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9595 - mse: 0.0086 - mae: 0.0614 - MAEMS: 0.9594 - val_loss: 2.0889 - val_mse: 0.0227 - val_mae: 0.1175 - val_MAEMS: 2.0899\n",
      "Epoch 39/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9348 - mse: 0.0083 - mae: 0.0600 - MAEMS: 0.9347 - val_loss: 2.0822 - val_mse: 0.0232 - val_mae: 0.1184 - val_MAEMS: 2.0824\n",
      "Epoch 40/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 0.9295 - mse: 0.0081 - mae: 0.0593 - MAEMS: 0.9294 - val_loss: 2.1701 - val_mse: 0.0228 - val_mae: 0.1185 - val_MAEMS: 2.1712\n",
      "Epoch 41/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9335 - mse: 0.0080 - mae: 0.0592 - MAEMS: 0.9334 - val_loss: 2.0874 - val_mse: 0.0230 - val_mae: 0.1181 - val_MAEMS: 2.0875\n",
      "Epoch 42/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9286 - mse: 0.0079 - mae: 0.0588 - MAEMS: 0.9286 - val_loss: 2.2174 - val_mse: 0.0225 - val_mae: 0.1176 - val_MAEMS: 2.2174\n",
      "Epoch 43/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9309 - mse: 0.0078 - mae: 0.0586 - MAEMS: 0.9309 - val_loss: 2.0378 - val_mse: 0.0243 - val_mae: 0.1208 - val_MAEMS: 2.0388\n",
      "Epoch 44/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9606 - mse: 0.0079 - mae: 0.0594 - MAEMS: 0.9606 - val_loss: 2.1726 - val_mse: 0.0226 - val_mae: 0.1174 - val_MAEMS: 2.1725\n",
      "Epoch 45/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 0.9373 - mse: 0.0077 - mae: 0.0585 - MAEMS: 0.9372 - val_loss: 2.1129 - val_mse: 0.0228 - val_mae: 0.1184 - val_MAEMS: 2.1134\n",
      "Epoch 46/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 0.8974 - mse: 0.0074 - mae: 0.0567 - MAEMS: 0.8973 - val_loss: 2.1032 - val_mse: 0.0225 - val_mae: 0.1174 - val_MAEMS: 2.1042\n",
      "Epoch 47/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.8873 - mse: 0.0072 - mae: 0.0559 - MAEMS: 0.8873 - val_loss: 2.1312 - val_mse: 0.0221 - val_mae: 0.1164 - val_MAEMS: 2.1321\n",
      "Epoch 48/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9079 - mse: 0.0073 - mae: 0.0564 - MAEMS: 0.9078 - val_loss: 2.0436 - val_mse: 0.0232 - val_mae: 0.1187 - val_MAEMS: 2.0446\n",
      "Epoch 49/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9438 - mse: 0.0075 - mae: 0.0579 - MAEMS: 0.9437 - val_loss: 2.0235 - val_mse: 0.0237 - val_mae: 0.1201 - val_MAEMS: 2.0253\n",
      "Epoch 50/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.9257 - mse: 0.0073 - mae: 0.0570 - MAEMS: 0.9256 - val_loss: 2.0772 - val_mse: 0.0230 - val_mae: 0.1189 - val_MAEMS: 2.0775\n",
      "Epoch 51/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 0.9046 - mse: 0.0071 - mae: 0.0558 - MAEMS: 0.9045 - val_loss: 2.0119 - val_mse: 0.0243 - val_mae: 0.1217 - val_MAEMS: 2.0123\n",
      "Epoch 52/1000\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 0.8793 - mse: 0.0069 - mae: 0.0546 - MAEMS: 0.8791 - val_loss: 2.0150 - val_mse: 0.0239 - val_mae: 0.1198 - val_MAEMS: 2.0160\n",
      "Epoch 53/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.8563 - mse: 0.0067 - mae: 0.0537 - MAEMS: 0.8563 - val_loss: 2.0702 - val_mse: 0.0228 - val_mae: 0.1177 - val_MAEMS: 2.0724\n",
      "Epoch 54/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.8588 - mse: 0.0066 - mae: 0.0534 - MAEMS: 0.8587 - val_loss: 2.0582 - val_mse: 0.0234 - val_mae: 0.1196 - val_MAEMS: 2.0598\n",
      "Epoch 55/1000\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.8414 - mse: 0.0064 - mae: 0.0525 - MAEMS: 0.8414 - val_loss: 2.1107 - val_mse: 0.0223 - val_mae: 0.1168 - val_MAEMS: 2.1120\n",
      "Wall time: 21min 25s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    b_size = 168\n",
    "    hist = model.fit(strvaX, strvaY, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(teX, teY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21e6f8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4601"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b5d3c",
   "metadata": {},
   "source": [
    "## Saving Results & Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c131f8fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loss_history = hist.history['loss']\n",
    "valloss_history = hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ec2ac20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('losshistory.txt',(loss_history,valloss_history))\n",
    "model.save('Basic Model Final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31187b3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "layer_name = 'attention_vec'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(strvaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f085a40e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 37)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_int_output = np.mean(intermediate_output, axis=0)\n",
    "avg_int_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d73435e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(np.transpose(abs(avg_int_output)), cmap=\"rocket\")\n",
    "lx = ax.set_xticklabels([])\n",
    "ly = ax.set_yticklabels([])\n",
    "cax = ax.figure.axes[-1]\n",
    "cax.tick_params(labelsize=14)\n",
    "\n",
    "f.savefig('attention_output.png', dpi=1000, bbox_inches=\"tight\")\n",
    "#f.savefig('attention_output.eps', dpi=1000, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f7927",
   "metadata": {},
   "source": [
    "## Basic Model Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1f6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 168\n",
    "testPredict = model.predict(teX, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e41ea0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npMAEMS(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true))*100\n",
    "def npMAEMD(y_true, y_pred):\n",
    "    return np.mean((abs(y_pred - y_true))*np.square(y_true-np.mean(y_true)))*100\n",
    "def npMSE(y_true, y_pred):\n",
    "    return np.mean(np.square(-y_true+y_pred))\n",
    "def npMAE(y_true, y_pred):\n",
    "    return np.mean(abs(-y_true+y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58c66aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.022304163678159232  MAE ==  0.11678586553605787  RMSE ==  0.5992415351992226\n"
     ]
    }
   ],
   "source": [
    "tePredict = testPredict.reshape(-1)\n",
    "testY = teY.reshape(-1)\n",
    "print('Error Test Score > MSE == ', npMSE(testY, tePredict), ' MAE == ', npMAE(testY, tePredict), ' MAEMD == ', npMAEMD(testY, tePredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c055c",
   "metadata": {},
   "source": [
    "## Wind Generation FFEL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trX, vaX, trY, vaY = train_test_split(trvaX, trvaY, test_size=1/6, shuffle=False)\n",
    "trY = trY.reshape(-1,output_timesteps)\n",
    "vaY = vaY.reshape(-1,output_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88280cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trX, batch_size=batch_size)\n",
    "validPredict = model.predict(vaX, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec9975c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31383, 24)\n",
      "(31383, 24)\n"
     ]
    }
   ],
   "source": [
    "e_tr = trainPredict - trY\n",
    "e_va = validPredict - vaY\n",
    "errors = np.vstack([e_tr, e_va])\n",
    "prediction = np.vstack([trainPredict, validPredict])\n",
    "print(errors.shape)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e950512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normalized Wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.644724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.721813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.714187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Normalized Wind\n",
       "0         0.644724\n",
       "1         0.658617\n",
       "2         0.683924\n",
       "3         0.721813\n",
       "4         0.714187"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df2 = pd.DataFrame(norm_df).iloc[:prediction.shape[0], :]\n",
    "norm_df2.columns = ['Normalized Wind']\n",
    "norm_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a2e79f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction1</th>\n",
       "      <th>Prediction2</th>\n",
       "      <th>Prediction3</th>\n",
       "      <th>Prediction4</th>\n",
       "      <th>Prediction5</th>\n",
       "      <th>Prediction6</th>\n",
       "      <th>Prediction7</th>\n",
       "      <th>Prediction8</th>\n",
       "      <th>Prediction9</th>\n",
       "      <th>Prediction10</th>\n",
       "      <th>...</th>\n",
       "      <th>Prediction15</th>\n",
       "      <th>Prediction16</th>\n",
       "      <th>Prediction17</th>\n",
       "      <th>Prediction18</th>\n",
       "      <th>Prediction19</th>\n",
       "      <th>Prediction20</th>\n",
       "      <th>Prediction21</th>\n",
       "      <th>Prediction22</th>\n",
       "      <th>Prediction23</th>\n",
       "      <th>Prediction24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.774323</td>\n",
       "      <td>0.741381</td>\n",
       "      <td>0.716587</td>\n",
       "      <td>0.704542</td>\n",
       "      <td>0.702208</td>\n",
       "      <td>0.711486</td>\n",
       "      <td>0.737553</td>\n",
       "      <td>0.760536</td>\n",
       "      <td>0.794829</td>\n",
       "      <td>0.826910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832913</td>\n",
       "      <td>0.828458</td>\n",
       "      <td>0.830511</td>\n",
       "      <td>0.853066</td>\n",
       "      <td>0.859240</td>\n",
       "      <td>0.882404</td>\n",
       "      <td>0.874423</td>\n",
       "      <td>0.865727</td>\n",
       "      <td>0.833718</td>\n",
       "      <td>0.795401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.764127</td>\n",
       "      <td>0.737782</td>\n",
       "      <td>0.717069</td>\n",
       "      <td>0.706137</td>\n",
       "      <td>0.706868</td>\n",
       "      <td>0.719355</td>\n",
       "      <td>0.743654</td>\n",
       "      <td>0.768242</td>\n",
       "      <td>0.804386</td>\n",
       "      <td>0.838177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835655</td>\n",
       "      <td>0.838367</td>\n",
       "      <td>0.847813</td>\n",
       "      <td>0.874759</td>\n",
       "      <td>0.879679</td>\n",
       "      <td>0.886548</td>\n",
       "      <td>0.871498</td>\n",
       "      <td>0.850968</td>\n",
       "      <td>0.815019</td>\n",
       "      <td>0.780045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.752948</td>\n",
       "      <td>0.734132</td>\n",
       "      <td>0.715556</td>\n",
       "      <td>0.709176</td>\n",
       "      <td>0.713454</td>\n",
       "      <td>0.730152</td>\n",
       "      <td>0.755751</td>\n",
       "      <td>0.778668</td>\n",
       "      <td>0.810655</td>\n",
       "      <td>0.836297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826571</td>\n",
       "      <td>0.839288</td>\n",
       "      <td>0.856169</td>\n",
       "      <td>0.881345</td>\n",
       "      <td>0.877601</td>\n",
       "      <td>0.862094</td>\n",
       "      <td>0.836676</td>\n",
       "      <td>0.805359</td>\n",
       "      <td>0.776427</td>\n",
       "      <td>0.753055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.729778</td>\n",
       "      <td>0.721467</td>\n",
       "      <td>0.710995</td>\n",
       "      <td>0.715731</td>\n",
       "      <td>0.727765</td>\n",
       "      <td>0.750957</td>\n",
       "      <td>0.778807</td>\n",
       "      <td>0.797437</td>\n",
       "      <td>0.821257</td>\n",
       "      <td>0.833073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827637</td>\n",
       "      <td>0.852442</td>\n",
       "      <td>0.866569</td>\n",
       "      <td>0.880208</td>\n",
       "      <td>0.862129</td>\n",
       "      <td>0.825838</td>\n",
       "      <td>0.791640</td>\n",
       "      <td>0.758721</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.732090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.722633</td>\n",
       "      <td>0.719223</td>\n",
       "      <td>0.715027</td>\n",
       "      <td>0.729678</td>\n",
       "      <td>0.749921</td>\n",
       "      <td>0.779671</td>\n",
       "      <td>0.809938</td>\n",
       "      <td>0.823806</td>\n",
       "      <td>0.839648</td>\n",
       "      <td>0.838092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846047</td>\n",
       "      <td>0.874661</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.877164</td>\n",
       "      <td>0.848665</td>\n",
       "      <td>0.801491</td>\n",
       "      <td>0.768298</td>\n",
       "      <td>0.740075</td>\n",
       "      <td>0.738649</td>\n",
       "      <td>0.733531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31378</th>\n",
       "      <td>0.376301</td>\n",
       "      <td>0.341228</td>\n",
       "      <td>0.319505</td>\n",
       "      <td>0.310303</td>\n",
       "      <td>0.314865</td>\n",
       "      <td>0.326736</td>\n",
       "      <td>0.352924</td>\n",
       "      <td>0.373706</td>\n",
       "      <td>0.408372</td>\n",
       "      <td>0.445486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598495</td>\n",
       "      <td>0.578913</td>\n",
       "      <td>0.547662</td>\n",
       "      <td>0.519708</td>\n",
       "      <td>0.490836</td>\n",
       "      <td>0.472754</td>\n",
       "      <td>0.459206</td>\n",
       "      <td>0.448720</td>\n",
       "      <td>0.437797</td>\n",
       "      <td>0.413605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31379</th>\n",
       "      <td>0.262524</td>\n",
       "      <td>0.235267</td>\n",
       "      <td>0.226003</td>\n",
       "      <td>0.225839</td>\n",
       "      <td>0.247815</td>\n",
       "      <td>0.276644</td>\n",
       "      <td>0.321980</td>\n",
       "      <td>0.359763</td>\n",
       "      <td>0.413376</td>\n",
       "      <td>0.463116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544097</td>\n",
       "      <td>0.522187</td>\n",
       "      <td>0.492307</td>\n",
       "      <td>0.477045</td>\n",
       "      <td>0.468451</td>\n",
       "      <td>0.454459</td>\n",
       "      <td>0.444752</td>\n",
       "      <td>0.424632</td>\n",
       "      <td>0.395457</td>\n",
       "      <td>0.348371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31380</th>\n",
       "      <td>0.198896</td>\n",
       "      <td>0.184548</td>\n",
       "      <td>0.189877</td>\n",
       "      <td>0.203966</td>\n",
       "      <td>0.243088</td>\n",
       "      <td>0.292439</td>\n",
       "      <td>0.358887</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.479625</td>\n",
       "      <td>0.530141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505905</td>\n",
       "      <td>0.487680</td>\n",
       "      <td>0.466984</td>\n",
       "      <td>0.461306</td>\n",
       "      <td>0.460752</td>\n",
       "      <td>0.438507</td>\n",
       "      <td>0.420627</td>\n",
       "      <td>0.389640</td>\n",
       "      <td>0.351018</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31381</th>\n",
       "      <td>0.192597</td>\n",
       "      <td>0.192868</td>\n",
       "      <td>0.211282</td>\n",
       "      <td>0.236224</td>\n",
       "      <td>0.281869</td>\n",
       "      <td>0.347085</td>\n",
       "      <td>0.425092</td>\n",
       "      <td>0.495405</td>\n",
       "      <td>0.557406</td>\n",
       "      <td>0.593775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493684</td>\n",
       "      <td>0.482293</td>\n",
       "      <td>0.476060</td>\n",
       "      <td>0.470693</td>\n",
       "      <td>0.467005</td>\n",
       "      <td>0.426835</td>\n",
       "      <td>0.396173</td>\n",
       "      <td>0.359456</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.278880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31382</th>\n",
       "      <td>0.227937</td>\n",
       "      <td>0.242173</td>\n",
       "      <td>0.270037</td>\n",
       "      <td>0.307649</td>\n",
       "      <td>0.357682</td>\n",
       "      <td>0.431161</td>\n",
       "      <td>0.510335</td>\n",
       "      <td>0.577777</td>\n",
       "      <td>0.625568</td>\n",
       "      <td>0.638250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508085</td>\n",
       "      <td>0.509003</td>\n",
       "      <td>0.517079</td>\n",
       "      <td>0.512484</td>\n",
       "      <td>0.494838</td>\n",
       "      <td>0.437993</td>\n",
       "      <td>0.390906</td>\n",
       "      <td>0.347868</td>\n",
       "      <td>0.319122</td>\n",
       "      <td>0.287640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31383 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Prediction1  Prediction2  Prediction3  Prediction4  Prediction5  \\\n",
       "0         0.774323     0.741381     0.716587     0.704542     0.702208   \n",
       "1         0.764127     0.737782     0.717069     0.706137     0.706868   \n",
       "2         0.752948     0.734132     0.715556     0.709176     0.713454   \n",
       "3         0.729778     0.721467     0.710995     0.715731     0.727765   \n",
       "4         0.722633     0.719223     0.715027     0.729678     0.749921   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "31378     0.376301     0.341228     0.319505     0.310303     0.314865   \n",
       "31379     0.262524     0.235267     0.226003     0.225839     0.247815   \n",
       "31380     0.198896     0.184548     0.189877     0.203966     0.243088   \n",
       "31381     0.192597     0.192868     0.211282     0.236224     0.281869   \n",
       "31382     0.227937     0.242173     0.270037     0.307649     0.357682   \n",
       "\n",
       "       Prediction6  Prediction7  Prediction8  Prediction9  Prediction10  ...  \\\n",
       "0         0.711486     0.737553     0.760536     0.794829      0.826910  ...   \n",
       "1         0.719355     0.743654     0.768242     0.804386      0.838177  ...   \n",
       "2         0.730152     0.755751     0.778668     0.810655      0.836297  ...   \n",
       "3         0.750957     0.778807     0.797437     0.821257      0.833073  ...   \n",
       "4         0.779671     0.809938     0.823806     0.839648      0.838092  ...   \n",
       "...            ...          ...          ...          ...           ...  ...   \n",
       "31378     0.326736     0.352924     0.373706     0.408372      0.445486  ...   \n",
       "31379     0.276644     0.321980     0.359763     0.413376      0.463116  ...   \n",
       "31380     0.292439     0.358887     0.417500     0.479625      0.530141  ...   \n",
       "31381     0.347085     0.425092     0.495405     0.557406      0.593775  ...   \n",
       "31382     0.431161     0.510335     0.577777     0.625568      0.638250  ...   \n",
       "\n",
       "       Prediction15  Prediction16  Prediction17  Prediction18  Prediction19  \\\n",
       "0          0.832913      0.828458      0.830511      0.853066      0.859240   \n",
       "1          0.835655      0.838367      0.847813      0.874759      0.879679   \n",
       "2          0.826571      0.839288      0.856169      0.881345      0.877601   \n",
       "3          0.827637      0.852442      0.866569      0.880208      0.862129   \n",
       "4          0.846047      0.874661      0.880321      0.877164      0.848665   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "31378      0.598495      0.578913      0.547662      0.519708      0.490836   \n",
       "31379      0.544097      0.522187      0.492307      0.477045      0.468451   \n",
       "31380      0.505905      0.487680      0.466984      0.461306      0.460752   \n",
       "31381      0.493684      0.482293      0.476060      0.470693      0.467005   \n",
       "31382      0.508085      0.509003      0.517079      0.512484      0.494838   \n",
       "\n",
       "       Prediction20  Prediction21  Prediction22  Prediction23  Prediction24  \n",
       "0          0.882404      0.874423      0.865727      0.833718      0.795401  \n",
       "1          0.886548      0.871498      0.850968      0.815019      0.780045  \n",
       "2          0.862094      0.836676      0.805359      0.776427      0.753055  \n",
       "3          0.825838      0.791640      0.758721      0.744271      0.732090  \n",
       "4          0.801491      0.768298      0.740075      0.738649      0.733531  \n",
       "...             ...           ...           ...           ...           ...  \n",
       "31378      0.472754      0.459206      0.448720      0.437797      0.413605  \n",
       "31379      0.454459      0.444752      0.424632      0.395457      0.348371  \n",
       "31380      0.438507      0.420627      0.389640      0.351018      0.297561  \n",
       "31381      0.426835      0.396173      0.359456      0.322311      0.278880  \n",
       "31382      0.437993      0.390906      0.347868      0.319122      0.287640  \n",
       "\n",
       "[31383 rows x 24 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prindex = ['Prediction1', 'Prediction2','Prediction3','Prediction4','Prediction5','Prediction6','Prediction7','Prediction8','Prediction9','Prediction10','Prediction11','Prediction12','Prediction13', 'Prediction14','Prediction15','Prediction16','Prediction17','Prediction18','Prediction19','Prediction20','Prediction21','Prediction22','Prediction23','Prediction24']\n",
    "Erindex = ['Error1', 'Error2','Error3','Error4','Error5','Error6','Error7','Error8','Error9','Error10','Error11','Error12','Error13', 'Error14','Error15','Error16','Error17','Error18','Error19','Error20','Error21','Error22','Error23','Error24']\n",
    "\n",
    "pr_df = pd.DataFrame(prediction, columns=Prindex)\n",
    "pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "351ca5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error1</th>\n",
       "      <th>Error2</th>\n",
       "      <th>Error3</th>\n",
       "      <th>Error4</th>\n",
       "      <th>Error5</th>\n",
       "      <th>Error6</th>\n",
       "      <th>Error7</th>\n",
       "      <th>Error8</th>\n",
       "      <th>Error9</th>\n",
       "      <th>Error10</th>\n",
       "      <th>...</th>\n",
       "      <th>Error15</th>\n",
       "      <th>Error16</th>\n",
       "      <th>Error17</th>\n",
       "      <th>Error18</th>\n",
       "      <th>Error19</th>\n",
       "      <th>Error20</th>\n",
       "      <th>Error21</th>\n",
       "      <th>Error22</th>\n",
       "      <th>Error23</th>\n",
       "      <th>Error24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.025016</td>\n",
       "      <td>-0.075785</td>\n",
       "      <td>-0.083432</td>\n",
       "      <td>-0.082604</td>\n",
       "      <td>-0.049139</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>-0.070092</td>\n",
       "      <td>-0.069356</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.041610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040843</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>-0.061995</td>\n",
       "      <td>-0.051778</td>\n",
       "      <td>-0.073923</td>\n",
       "      <td>-0.054742</td>\n",
       "      <td>-0.080259</td>\n",
       "      <td>-0.101488</td>\n",
       "      <td>-0.105032</td>\n",
       "      <td>-0.083052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.053039</td>\n",
       "      <td>-0.062237</td>\n",
       "      <td>-0.070078</td>\n",
       "      <td>-0.045209</td>\n",
       "      <td>-0.041467</td>\n",
       "      <td>-0.088291</td>\n",
       "      <td>-0.086239</td>\n",
       "      <td>-0.008169</td>\n",
       "      <td>0.019085</td>\n",
       "      <td>-0.029945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053062</td>\n",
       "      <td>-0.054139</td>\n",
       "      <td>-0.057031</td>\n",
       "      <td>-0.058404</td>\n",
       "      <td>-0.057467</td>\n",
       "      <td>-0.068135</td>\n",
       "      <td>-0.095716</td>\n",
       "      <td>-0.087782</td>\n",
       "      <td>-0.063434</td>\n",
       "      <td>-0.081774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.047071</td>\n",
       "      <td>-0.053014</td>\n",
       "      <td>-0.035790</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>-0.094191</td>\n",
       "      <td>-0.099740</td>\n",
       "      <td>-0.020661</td>\n",
       "      <td>-0.006633</td>\n",
       "      <td>-0.057466</td>\n",
       "      <td>-0.024538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065935</td>\n",
       "      <td>-0.065556</td>\n",
       "      <td>-0.076994</td>\n",
       "      <td>-0.055802</td>\n",
       "      <td>-0.077081</td>\n",
       "      <td>-0.105121</td>\n",
       "      <td>-0.102074</td>\n",
       "      <td>-0.073095</td>\n",
       "      <td>-0.085392</td>\n",
       "      <td>-0.087067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.057368</td>\n",
       "      <td>-0.029879</td>\n",
       "      <td>-0.037340</td>\n",
       "      <td>-0.091914</td>\n",
       "      <td>-0.102128</td>\n",
       "      <td>-0.025454</td>\n",
       "      <td>-0.006493</td>\n",
       "      <td>-0.070684</td>\n",
       "      <td>-0.039578</td>\n",
       "      <td>-0.027131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077207</td>\n",
       "      <td>-0.080721</td>\n",
       "      <td>-0.070578</td>\n",
       "      <td>-0.074474</td>\n",
       "      <td>-0.105086</td>\n",
       "      <td>-0.112911</td>\n",
       "      <td>-0.086813</td>\n",
       "      <td>-0.103097</td>\n",
       "      <td>-0.095851</td>\n",
       "      <td>-0.095448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028714</td>\n",
       "      <td>-0.029112</td>\n",
       "      <td>-0.092618</td>\n",
       "      <td>-0.100215</td>\n",
       "      <td>-0.026491</td>\n",
       "      <td>-0.005630</td>\n",
       "      <td>-0.058183</td>\n",
       "      <td>-0.037029</td>\n",
       "      <td>-0.020556</td>\n",
       "      <td>-0.029058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087116</td>\n",
       "      <td>-0.062485</td>\n",
       "      <td>-0.074361</td>\n",
       "      <td>-0.090051</td>\n",
       "      <td>-0.090085</td>\n",
       "      <td>-0.076962</td>\n",
       "      <td>-0.093521</td>\n",
       "      <td>-0.100047</td>\n",
       "      <td>-0.088889</td>\n",
       "      <td>-0.076746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31378</th>\n",
       "      <td>0.005052</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>0.056190</td>\n",
       "      <td>0.063513</td>\n",
       "      <td>0.061451</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.029751</td>\n",
       "      <td>-0.008594</td>\n",
       "      <td>-0.037586</td>\n",
       "      <td>-0.069082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154837</td>\n",
       "      <td>0.169037</td>\n",
       "      <td>0.052722</td>\n",
       "      <td>-0.007410</td>\n",
       "      <td>-0.006683</td>\n",
       "      <td>-0.019815</td>\n",
       "      <td>-0.049574</td>\n",
       "      <td>-0.027185</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>0.018510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31379</th>\n",
       "      <td>-0.052770</td>\n",
       "      <td>-0.028048</td>\n",
       "      <td>-0.020787</td>\n",
       "      <td>-0.027575</td>\n",
       "      <td>-0.024948</td>\n",
       "      <td>-0.046530</td>\n",
       "      <td>-0.060320</td>\n",
       "      <td>-0.086196</td>\n",
       "      <td>-0.101191</td>\n",
       "      <td>-0.125533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134221</td>\n",
       "      <td>0.027248</td>\n",
       "      <td>-0.034811</td>\n",
       "      <td>-0.020475</td>\n",
       "      <td>-0.024119</td>\n",
       "      <td>-0.054321</td>\n",
       "      <td>-0.031153</td>\n",
       "      <td>-0.040117</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.032972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31380</th>\n",
       "      <td>-0.064419</td>\n",
       "      <td>-0.062242</td>\n",
       "      <td>-0.063537</td>\n",
       "      <td>-0.068797</td>\n",
       "      <td>-0.080085</td>\n",
       "      <td>-0.089861</td>\n",
       "      <td>-0.087071</td>\n",
       "      <td>-0.097068</td>\n",
       "      <td>-0.109024</td>\n",
       "      <td>-0.098426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>-0.039438</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>-0.031263</td>\n",
       "      <td>-0.048028</td>\n",
       "      <td>-0.037398</td>\n",
       "      <td>-0.044122</td>\n",
       "      <td>-0.005455</td>\n",
       "      <td>0.035619</td>\n",
       "      <td>0.039266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31381</th>\n",
       "      <td>-0.054194</td>\n",
       "      <td>-0.060546</td>\n",
       "      <td>-0.061481</td>\n",
       "      <td>-0.086949</td>\n",
       "      <td>-0.100431</td>\n",
       "      <td>-0.098874</td>\n",
       "      <td>-0.089475</td>\n",
       "      <td>-0.093245</td>\n",
       "      <td>-0.071161</td>\n",
       "      <td>-0.020603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>-0.015227</td>\n",
       "      <td>-0.016509</td>\n",
       "      <td>-0.038087</td>\n",
       "      <td>-0.008900</td>\n",
       "      <td>-0.037914</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.044057</td>\n",
       "      <td>0.064016</td>\n",
       "      <td>0.052903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31382</th>\n",
       "      <td>-0.025477</td>\n",
       "      <td>-0.030590</td>\n",
       "      <td>-0.053136</td>\n",
       "      <td>-0.074651</td>\n",
       "      <td>-0.088276</td>\n",
       "      <td>-0.083406</td>\n",
       "      <td>-0.078314</td>\n",
       "      <td>-0.050790</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>0.098861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010566</td>\n",
       "      <td>0.016434</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.036579</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.042899</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.089573</td>\n",
       "      <td>0.093145</td>\n",
       "      <td>0.048241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31383 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Error1    Error2    Error3    Error4    Error5    Error6    Error7  \\\n",
       "0     -0.025016 -0.075785 -0.083432 -0.082604 -0.049139 -0.036849 -0.070092   \n",
       "1     -0.053039 -0.062237 -0.070078 -0.045209 -0.041467 -0.088291 -0.086239   \n",
       "2     -0.047071 -0.053014 -0.035790 -0.039159 -0.094191 -0.099740 -0.020661   \n",
       "3     -0.057368 -0.029879 -0.037340 -0.091914 -0.102128 -0.025454 -0.006493   \n",
       "4     -0.028714 -0.029112 -0.092618 -0.100215 -0.026491 -0.005630 -0.058183   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "31378  0.005052  0.025933  0.056190  0.063513  0.061451  0.053974  0.029751   \n",
       "31379 -0.052770 -0.028048 -0.020787 -0.027575 -0.024948 -0.046530 -0.060320   \n",
       "31380 -0.064419 -0.062242 -0.063537 -0.068797 -0.080085 -0.089861 -0.087071   \n",
       "31381 -0.054194 -0.060546 -0.061481 -0.086949 -0.100431 -0.098874 -0.089475   \n",
       "31382 -0.025477 -0.030590 -0.053136 -0.074651 -0.088276 -0.083406 -0.078314   \n",
       "\n",
       "         Error8    Error9   Error10  ...   Error15   Error16   Error17  \\\n",
       "0     -0.069356  0.018418  0.041610  ... -0.040843 -0.060259 -0.061995   \n",
       "1     -0.008169  0.019085 -0.029945  ... -0.053062 -0.054139 -0.057031   \n",
       "2     -0.006633 -0.057466 -0.024538  ... -0.065935 -0.065556 -0.076994   \n",
       "3     -0.070684 -0.039578 -0.027131  ... -0.077207 -0.080721 -0.070578   \n",
       "4     -0.037029 -0.020556 -0.029058  ... -0.087116 -0.062485 -0.074361   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "31378 -0.008594 -0.037586 -0.069082  ...  0.154837  0.169037  0.052722   \n",
       "31379 -0.086196 -0.101191 -0.125533  ...  0.134221  0.027248 -0.034811   \n",
       "31380 -0.097068 -0.109024 -0.098426  ...  0.010965 -0.039438 -0.030536   \n",
       "31381 -0.093245 -0.071161 -0.020603  ... -0.033434 -0.015227 -0.016509   \n",
       "31382 -0.050790  0.011190  0.098861  ...  0.010566  0.016434  0.008299   \n",
       "\n",
       "        Error18   Error19   Error20   Error21   Error22   Error23   Error24  \n",
       "0     -0.051778 -0.073923 -0.054742 -0.080259 -0.101488 -0.105032 -0.083052  \n",
       "1     -0.058404 -0.057467 -0.068135 -0.095716 -0.087782 -0.063434 -0.081774  \n",
       "2     -0.055802 -0.077081 -0.105121 -0.102074 -0.073095 -0.085392 -0.087067  \n",
       "3     -0.074474 -0.105086 -0.112911 -0.086813 -0.103097 -0.095851 -0.095448  \n",
       "4     -0.090051 -0.090085 -0.076962 -0.093521 -0.100047 -0.088889 -0.076746  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "31378 -0.007410 -0.006683 -0.019815 -0.049574 -0.027185 -0.026952  0.018510  \n",
       "31379 -0.020475 -0.024119 -0.054321 -0.031153 -0.040117  0.000363  0.032972  \n",
       "31380 -0.031263 -0.048028 -0.037398 -0.044122 -0.005455  0.035619  0.039266  \n",
       "31381 -0.038087 -0.008900 -0.037914  0.001079  0.044057  0.064016  0.052903  \n",
       "31382  0.036579  0.030089  0.042899  0.075506  0.089573  0.093145  0.048241  \n",
       "\n",
       "[31383 rows x 24 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er_df = pd.DataFrame(errors, columns=Erindex)\n",
    "er_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a614db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normalized Wind</th>\n",
       "      <th>Prediction1</th>\n",
       "      <th>Prediction2</th>\n",
       "      <th>Prediction3</th>\n",
       "      <th>Prediction4</th>\n",
       "      <th>Prediction5</th>\n",
       "      <th>Prediction6</th>\n",
       "      <th>Prediction7</th>\n",
       "      <th>Prediction8</th>\n",
       "      <th>Prediction9</th>\n",
       "      <th>...</th>\n",
       "      <th>Error15</th>\n",
       "      <th>Error16</th>\n",
       "      <th>Error17</th>\n",
       "      <th>Error18</th>\n",
       "      <th>Error19</th>\n",
       "      <th>Error20</th>\n",
       "      <th>Error21</th>\n",
       "      <th>Error22</th>\n",
       "      <th>Error23</th>\n",
       "      <th>Error24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.644724</td>\n",
       "      <td>0.774323</td>\n",
       "      <td>0.741381</td>\n",
       "      <td>0.716587</td>\n",
       "      <td>0.704542</td>\n",
       "      <td>0.702208</td>\n",
       "      <td>0.711486</td>\n",
       "      <td>0.737553</td>\n",
       "      <td>0.760536</td>\n",
       "      <td>0.794829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040843</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>-0.061995</td>\n",
       "      <td>-0.051778</td>\n",
       "      <td>-0.073923</td>\n",
       "      <td>-0.054742</td>\n",
       "      <td>-0.080259</td>\n",
       "      <td>-0.101488</td>\n",
       "      <td>-0.105032</td>\n",
       "      <td>-0.083052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.658617</td>\n",
       "      <td>0.764127</td>\n",
       "      <td>0.737782</td>\n",
       "      <td>0.717069</td>\n",
       "      <td>0.706137</td>\n",
       "      <td>0.706868</td>\n",
       "      <td>0.719355</td>\n",
       "      <td>0.743654</td>\n",
       "      <td>0.768242</td>\n",
       "      <td>0.804386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053062</td>\n",
       "      <td>-0.054139</td>\n",
       "      <td>-0.057031</td>\n",
       "      <td>-0.058404</td>\n",
       "      <td>-0.057467</td>\n",
       "      <td>-0.068135</td>\n",
       "      <td>-0.095716</td>\n",
       "      <td>-0.087782</td>\n",
       "      <td>-0.063434</td>\n",
       "      <td>-0.081774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.683924</td>\n",
       "      <td>0.752948</td>\n",
       "      <td>0.734132</td>\n",
       "      <td>0.715556</td>\n",
       "      <td>0.709176</td>\n",
       "      <td>0.713454</td>\n",
       "      <td>0.730152</td>\n",
       "      <td>0.755751</td>\n",
       "      <td>0.778668</td>\n",
       "      <td>0.810655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065935</td>\n",
       "      <td>-0.065556</td>\n",
       "      <td>-0.076994</td>\n",
       "      <td>-0.055802</td>\n",
       "      <td>-0.077081</td>\n",
       "      <td>-0.105121</td>\n",
       "      <td>-0.102074</td>\n",
       "      <td>-0.073095</td>\n",
       "      <td>-0.085392</td>\n",
       "      <td>-0.087067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.721813</td>\n",
       "      <td>0.729778</td>\n",
       "      <td>0.721467</td>\n",
       "      <td>0.710995</td>\n",
       "      <td>0.715731</td>\n",
       "      <td>0.727765</td>\n",
       "      <td>0.750957</td>\n",
       "      <td>0.778807</td>\n",
       "      <td>0.797437</td>\n",
       "      <td>0.821257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077207</td>\n",
       "      <td>-0.080721</td>\n",
       "      <td>-0.070578</td>\n",
       "      <td>-0.074474</td>\n",
       "      <td>-0.105086</td>\n",
       "      <td>-0.112911</td>\n",
       "      <td>-0.086813</td>\n",
       "      <td>-0.103097</td>\n",
       "      <td>-0.095851</td>\n",
       "      <td>-0.095448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.714187</td>\n",
       "      <td>0.722633</td>\n",
       "      <td>0.719223</td>\n",
       "      <td>0.715027</td>\n",
       "      <td>0.729678</td>\n",
       "      <td>0.749921</td>\n",
       "      <td>0.779671</td>\n",
       "      <td>0.809938</td>\n",
       "      <td>0.823806</td>\n",
       "      <td>0.839648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087116</td>\n",
       "      <td>-0.062485</td>\n",
       "      <td>-0.074361</td>\n",
       "      <td>-0.090051</td>\n",
       "      <td>-0.090085</td>\n",
       "      <td>-0.076962</td>\n",
       "      <td>-0.093521</td>\n",
       "      <td>-0.100047</td>\n",
       "      <td>-0.088889</td>\n",
       "      <td>-0.076746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Normalized Wind  Prediction1  Prediction2  Prediction3  Prediction4  \\\n",
       "0         0.644724     0.774323     0.741381     0.716587     0.704542   \n",
       "1         0.658617     0.764127     0.737782     0.717069     0.706137   \n",
       "2         0.683924     0.752948     0.734132     0.715556     0.709176   \n",
       "3         0.721813     0.729778     0.721467     0.710995     0.715731   \n",
       "4         0.714187     0.722633     0.719223     0.715027     0.729678   \n",
       "\n",
       "   Prediction5  Prediction6  Prediction7  Prediction8  Prediction9  ...  \\\n",
       "0     0.702208     0.711486     0.737553     0.760536     0.794829  ...   \n",
       "1     0.706868     0.719355     0.743654     0.768242     0.804386  ...   \n",
       "2     0.713454     0.730152     0.755751     0.778668     0.810655  ...   \n",
       "3     0.727765     0.750957     0.778807     0.797437     0.821257  ...   \n",
       "4     0.749921     0.779671     0.809938     0.823806     0.839648  ...   \n",
       "\n",
       "    Error15   Error16   Error17   Error18   Error19   Error20   Error21  \\\n",
       "0 -0.040843 -0.060259 -0.061995 -0.051778 -0.073923 -0.054742 -0.080259   \n",
       "1 -0.053062 -0.054139 -0.057031 -0.058404 -0.057467 -0.068135 -0.095716   \n",
       "2 -0.065935 -0.065556 -0.076994 -0.055802 -0.077081 -0.105121 -0.102074   \n",
       "3 -0.077207 -0.080721 -0.070578 -0.074474 -0.105086 -0.112911 -0.086813   \n",
       "4 -0.087116 -0.062485 -0.074361 -0.090051 -0.090085 -0.076962 -0.093521   \n",
       "\n",
       "    Error22   Error23   Error24  \n",
       "0 -0.101488 -0.105032 -0.083052  \n",
       "1 -0.087782 -0.063434 -0.081774  \n",
       "2 -0.073095 -0.085392 -0.087067  \n",
       "3 -0.103097 -0.095851 -0.095448  \n",
       "4 -0.100047 -0.088889 -0.076746  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df2 = pd.concat([norm_df2, pr_df, er_df],axis=1)\n",
    "norm_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4f80893",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df22 = pd.DataFrame(norm_df).iloc[prediction.shape[0]+timesteps:, :]\n",
    "norm_df22.columns = ['Normalized Wind']\n",
    "npnorm22 = np.array(norm_df22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15241237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247611</td>\n",
       "      <td>0.264004</td>\n",
       "      <td>0.295378</td>\n",
       "      <td>0.340294</td>\n",
       "      <td>0.397206</td>\n",
       "      <td>0.476981</td>\n",
       "      <td>0.555980</td>\n",
       "      <td>0.612252</td>\n",
       "      <td>0.644406</td>\n",
       "      <td>0.639012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528575</td>\n",
       "      <td>0.535129</td>\n",
       "      <td>0.544030</td>\n",
       "      <td>0.532309</td>\n",
       "      <td>0.495391</td>\n",
       "      <td>0.424992</td>\n",
       "      <td>0.368086</td>\n",
       "      <td>0.324650</td>\n",
       "      <td>0.306843</td>\n",
       "      <td>0.284149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.259947</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>0.321609</td>\n",
       "      <td>0.381457</td>\n",
       "      <td>0.452726</td>\n",
       "      <td>0.535357</td>\n",
       "      <td>0.609359</td>\n",
       "      <td>0.649634</td>\n",
       "      <td>0.657759</td>\n",
       "      <td>0.636575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555947</td>\n",
       "      <td>0.562837</td>\n",
       "      <td>0.561606</td>\n",
       "      <td>0.536782</td>\n",
       "      <td>0.484978</td>\n",
       "      <td>0.411580</td>\n",
       "      <td>0.357064</td>\n",
       "      <td>0.319764</td>\n",
       "      <td>0.313551</td>\n",
       "      <td>0.299435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.317199</td>\n",
       "      <td>0.347193</td>\n",
       "      <td>0.394371</td>\n",
       "      <td>0.460599</td>\n",
       "      <td>0.533121</td>\n",
       "      <td>0.599056</td>\n",
       "      <td>0.652459</td>\n",
       "      <td>0.668103</td>\n",
       "      <td>0.650857</td>\n",
       "      <td>0.621986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577066</td>\n",
       "      <td>0.573343</td>\n",
       "      <td>0.552232</td>\n",
       "      <td>0.515163</td>\n",
       "      <td>0.456278</td>\n",
       "      <td>0.395760</td>\n",
       "      <td>0.356408</td>\n",
       "      <td>0.336366</td>\n",
       "      <td>0.348964</td>\n",
       "      <td>0.346879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.406243</td>\n",
       "      <td>0.446613</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>0.557627</td>\n",
       "      <td>0.608931</td>\n",
       "      <td>0.643542</td>\n",
       "      <td>0.662986</td>\n",
       "      <td>0.651456</td>\n",
       "      <td>0.618666</td>\n",
       "      <td>0.592764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567849</td>\n",
       "      <td>0.550273</td>\n",
       "      <td>0.517917</td>\n",
       "      <td>0.479423</td>\n",
       "      <td>0.433545</td>\n",
       "      <td>0.395397</td>\n",
       "      <td>0.371723</td>\n",
       "      <td>0.370379</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.409072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499609</td>\n",
       "      <td>0.557822</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.651419</td>\n",
       "      <td>0.666574</td>\n",
       "      <td>0.662269</td>\n",
       "      <td>0.647671</td>\n",
       "      <td>0.618445</td>\n",
       "      <td>0.589127</td>\n",
       "      <td>0.570585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538921</td>\n",
       "      <td>0.513423</td>\n",
       "      <td>0.477917</td>\n",
       "      <td>0.448328</td>\n",
       "      <td>0.423249</td>\n",
       "      <td>0.402825</td>\n",
       "      <td>0.388871</td>\n",
       "      <td>0.392530</td>\n",
       "      <td>0.420063</td>\n",
       "      <td>0.444472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>0.284534</td>\n",
       "      <td>0.343144</td>\n",
       "      <td>0.402274</td>\n",
       "      <td>0.442910</td>\n",
       "      <td>0.455449</td>\n",
       "      <td>0.461330</td>\n",
       "      <td>0.470786</td>\n",
       "      <td>0.470959</td>\n",
       "      <td>0.466618</td>\n",
       "      <td>0.459793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423174</td>\n",
       "      <td>0.424606</td>\n",
       "      <td>0.423632</td>\n",
       "      <td>0.424239</td>\n",
       "      <td>0.431224</td>\n",
       "      <td>0.432175</td>\n",
       "      <td>0.442301</td>\n",
       "      <td>0.467610</td>\n",
       "      <td>0.491154</td>\n",
       "      <td>0.496676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>0.337997</td>\n",
       "      <td>0.384219</td>\n",
       "      <td>0.428044</td>\n",
       "      <td>0.450970</td>\n",
       "      <td>0.446364</td>\n",
       "      <td>0.442964</td>\n",
       "      <td>0.445539</td>\n",
       "      <td>0.449591</td>\n",
       "      <td>0.446811</td>\n",
       "      <td>0.438624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401414</td>\n",
       "      <td>0.405412</td>\n",
       "      <td>0.405174</td>\n",
       "      <td>0.411310</td>\n",
       "      <td>0.414890</td>\n",
       "      <td>0.412157</td>\n",
       "      <td>0.420535</td>\n",
       "      <td>0.443814</td>\n",
       "      <td>0.474875</td>\n",
       "      <td>0.483649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>0.426191</td>\n",
       "      <td>0.444796</td>\n",
       "      <td>0.457980</td>\n",
       "      <td>0.449922</td>\n",
       "      <td>0.424813</td>\n",
       "      <td>0.415106</td>\n",
       "      <td>0.416662</td>\n",
       "      <td>0.427175</td>\n",
       "      <td>0.429480</td>\n",
       "      <td>0.421655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380661</td>\n",
       "      <td>0.393294</td>\n",
       "      <td>0.399049</td>\n",
       "      <td>0.409087</td>\n",
       "      <td>0.405630</td>\n",
       "      <td>0.394243</td>\n",
       "      <td>0.396697</td>\n",
       "      <td>0.414684</td>\n",
       "      <td>0.450230</td>\n",
       "      <td>0.466181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>0.520418</td>\n",
       "      <td>0.507098</td>\n",
       "      <td>0.485686</td>\n",
       "      <td>0.448044</td>\n",
       "      <td>0.408167</td>\n",
       "      <td>0.397024</td>\n",
       "      <td>0.401413</td>\n",
       "      <td>0.417858</td>\n",
       "      <td>0.423466</td>\n",
       "      <td>0.419017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394453</td>\n",
       "      <td>0.417433</td>\n",
       "      <td>0.430702</td>\n",
       "      <td>0.441706</td>\n",
       "      <td>0.436794</td>\n",
       "      <td>0.421682</td>\n",
       "      <td>0.421983</td>\n",
       "      <td>0.437653</td>\n",
       "      <td>0.473951</td>\n",
       "      <td>0.492936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>0.598868</td>\n",
       "      <td>0.559919</td>\n",
       "      <td>0.512338</td>\n",
       "      <td>0.456348</td>\n",
       "      <td>0.409025</td>\n",
       "      <td>0.399422</td>\n",
       "      <td>0.403258</td>\n",
       "      <td>0.421270</td>\n",
       "      <td>0.425990</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441948</td>\n",
       "      <td>0.475280</td>\n",
       "      <td>0.493884</td>\n",
       "      <td>0.501058</td>\n",
       "      <td>0.493787</td>\n",
       "      <td>0.476592</td>\n",
       "      <td>0.474356</td>\n",
       "      <td>0.486164</td>\n",
       "      <td>0.519420</td>\n",
       "      <td>0.530787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3487 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.247611  0.264004  0.295378  0.340294  0.397206  0.476981  0.555980   \n",
       "1     0.259947  0.282110  0.321609  0.381457  0.452726  0.535357  0.609359   \n",
       "2     0.317199  0.347193  0.394371  0.460599  0.533121  0.599056  0.652459   \n",
       "3     0.406243  0.446613  0.498657  0.557627  0.608931  0.643542  0.662986   \n",
       "4     0.499609  0.557822  0.609756  0.651419  0.666574  0.662269  0.647671   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3482  0.284534  0.343144  0.402274  0.442910  0.455449  0.461330  0.470786   \n",
       "3483  0.337997  0.384219  0.428044  0.450970  0.446364  0.442964  0.445539   \n",
       "3484  0.426191  0.444796  0.457980  0.449922  0.424813  0.415106  0.416662   \n",
       "3485  0.520418  0.507098  0.485686  0.448044  0.408167  0.397024  0.401413   \n",
       "3486  0.598868  0.559919  0.512338  0.456348  0.409025  0.399422  0.403258   \n",
       "\n",
       "            7         8         9   ...        14        15        16  \\\n",
       "0     0.612252  0.644406  0.639012  ...  0.528575  0.535129  0.544030   \n",
       "1     0.649634  0.657759  0.636575  ...  0.555947  0.562837  0.561606   \n",
       "2     0.668103  0.650857  0.621986  ...  0.577066  0.573343  0.552232   \n",
       "3     0.651456  0.618666  0.592764  ...  0.567849  0.550273  0.517917   \n",
       "4     0.618445  0.589127  0.570585  ...  0.538921  0.513423  0.477917   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3482  0.470959  0.466618  0.459793  ...  0.423174  0.424606  0.423632   \n",
       "3483  0.449591  0.446811  0.438624  ...  0.401414  0.405412  0.405174   \n",
       "3484  0.427175  0.429480  0.421655  ...  0.380661  0.393294  0.399049   \n",
       "3485  0.417858  0.423466  0.419017  ...  0.394453  0.417433  0.430702   \n",
       "3486  0.421270  0.425990  0.420894  ...  0.441948  0.475280  0.493884   \n",
       "\n",
       "            17        18        19        20        21        22        23  \n",
       "0     0.532309  0.495391  0.424992  0.368086  0.324650  0.306843  0.284149  \n",
       "1     0.536782  0.484978  0.411580  0.357064  0.319764  0.313551  0.299435  \n",
       "2     0.515163  0.456278  0.395760  0.356408  0.336366  0.348964  0.346879  \n",
       "3     0.479423  0.433545  0.395397  0.371723  0.370379  0.397059  0.409072  \n",
       "4     0.448328  0.423249  0.402825  0.388871  0.392530  0.420063  0.444472  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3482  0.424239  0.431224  0.432175  0.442301  0.467610  0.491154  0.496676  \n",
       "3483  0.411310  0.414890  0.412157  0.420535  0.443814  0.474875  0.483649  \n",
       "3484  0.409087  0.405630  0.394243  0.396697  0.414684  0.450230  0.466181  \n",
       "3485  0.441706  0.436794  0.421682  0.421983  0.437653  0.473951  0.492936  \n",
       "3486  0.501058  0.493787  0.476592  0.474356  0.486164  0.519420  0.530787  \n",
       "\n",
       "[3487 rows x 24 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_df = pd.DataFrame(tePredict.reshape(-1,24))\n",
    "pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "402bd749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error1</th>\n",
       "      <th>Error2</th>\n",
       "      <th>Error3</th>\n",
       "      <th>Error4</th>\n",
       "      <th>Error5</th>\n",
       "      <th>Error6</th>\n",
       "      <th>Error7</th>\n",
       "      <th>Error8</th>\n",
       "      <th>Error9</th>\n",
       "      <th>Error10</th>\n",
       "      <th>...</th>\n",
       "      <th>Error15</th>\n",
       "      <th>Error16</th>\n",
       "      <th>Error17</th>\n",
       "      <th>Error18</th>\n",
       "      <th>Error19</th>\n",
       "      <th>Error20</th>\n",
       "      <th>Error21</th>\n",
       "      <th>Error22</th>\n",
       "      <th>Error23</th>\n",
       "      <th>Error24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.025152</td>\n",
       "      <td>-0.059169</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>-0.105664</td>\n",
       "      <td>-0.117361</td>\n",
       "      <td>-0.111668</td>\n",
       "      <td>-0.072587</td>\n",
       "      <td>-0.002126</td>\n",
       "      <td>0.105017</td>\n",
       "      <td>0.195354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036005</td>\n",
       "      <td>0.026349</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.067560</td>\n",
       "      <td>0.100296</td>\n",
       "      <td>0.109592</td>\n",
       "      <td>0.109791</td>\n",
       "      <td>0.098673</td>\n",
       "      <td>0.067444</td>\n",
       "      <td>0.031502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.063227</td>\n",
       "      <td>-0.100190</td>\n",
       "      <td>-0.124349</td>\n",
       "      <td>-0.133111</td>\n",
       "      <td>-0.135924</td>\n",
       "      <td>-0.093210</td>\n",
       "      <td>-0.005019</td>\n",
       "      <td>0.110245</td>\n",
       "      <td>0.214102</td>\n",
       "      <td>0.226699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047167</td>\n",
       "      <td>0.086932</td>\n",
       "      <td>0.096857</td>\n",
       "      <td>0.141688</td>\n",
       "      <td>0.169579</td>\n",
       "      <td>0.153286</td>\n",
       "      <td>0.131086</td>\n",
       "      <td>0.080365</td>\n",
       "      <td>0.060904</td>\n",
       "      <td>0.022698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.065101</td>\n",
       "      <td>-0.098766</td>\n",
       "      <td>-0.120196</td>\n",
       "      <td>-0.128050</td>\n",
       "      <td>-0.095446</td>\n",
       "      <td>-0.015322</td>\n",
       "      <td>0.113070</td>\n",
       "      <td>0.224445</td>\n",
       "      <td>0.240981</td>\n",
       "      <td>0.127046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101161</td>\n",
       "      <td>0.108593</td>\n",
       "      <td>0.157137</td>\n",
       "      <td>0.199763</td>\n",
       "      <td>0.197983</td>\n",
       "      <td>0.169782</td>\n",
       "      <td>0.117008</td>\n",
       "      <td>0.083719</td>\n",
       "      <td>0.072227</td>\n",
       "      <td>0.009551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>-0.067955</td>\n",
       "      <td>-0.089993</td>\n",
       "      <td>-0.070940</td>\n",
       "      <td>-0.005447</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.219329</td>\n",
       "      <td>0.241580</td>\n",
       "      <td>0.123726</td>\n",
       "      <td>0.065646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.155179</td>\n",
       "      <td>0.202517</td>\n",
       "      <td>0.221129</td>\n",
       "      <td>0.207567</td>\n",
       "      <td>0.155997</td>\n",
       "      <td>0.119076</td>\n",
       "      <td>0.093642</td>\n",
       "      <td>0.059731</td>\n",
       "      <td>0.024925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.014958</td>\n",
       "      <td>-0.030828</td>\n",
       "      <td>-0.018811</td>\n",
       "      <td>0.037041</td>\n",
       "      <td>0.127185</td>\n",
       "      <td>0.218611</td>\n",
       "      <td>0.237795</td>\n",
       "      <td>0.123505</td>\n",
       "      <td>0.062009</td>\n",
       "      <td>0.073066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143826</td>\n",
       "      <td>0.198024</td>\n",
       "      <td>0.219623</td>\n",
       "      <td>0.222350</td>\n",
       "      <td>0.183849</td>\n",
       "      <td>0.150177</td>\n",
       "      <td>0.112134</td>\n",
       "      <td>0.055202</td>\n",
       "      <td>0.035915</td>\n",
       "      <td>0.029576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>0.054111</td>\n",
       "      <td>-0.032766</td>\n",
       "      <td>-0.126196</td>\n",
       "      <td>-0.209254</td>\n",
       "      <td>-0.295928</td>\n",
       "      <td>-0.338872</td>\n",
       "      <td>-0.351584</td>\n",
       "      <td>-0.334105</td>\n",
       "      <td>-0.304751</td>\n",
       "      <td>-0.271728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048482</td>\n",
       "      <td>-0.008425</td>\n",
       "      <td>0.120448</td>\n",
       "      <td>0.145196</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>0.134227</td>\n",
       "      <td>0.160606</td>\n",
       "      <td>0.174015</td>\n",
       "      <td>0.179981</td>\n",
       "      <td>0.155651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>-0.037913</td>\n",
       "      <td>-0.144251</td>\n",
       "      <td>-0.224120</td>\n",
       "      <td>-0.300407</td>\n",
       "      <td>-0.353838</td>\n",
       "      <td>-0.379406</td>\n",
       "      <td>-0.359524</td>\n",
       "      <td>-0.321779</td>\n",
       "      <td>-0.284710</td>\n",
       "      <td>-0.260562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.102229</td>\n",
       "      <td>0.126131</td>\n",
       "      <td>0.096329</td>\n",
       "      <td>0.116942</td>\n",
       "      <td>0.130462</td>\n",
       "      <td>0.126939</td>\n",
       "      <td>0.132640</td>\n",
       "      <td>0.133849</td>\n",
       "      <td>0.112907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>-0.102279</td>\n",
       "      <td>-0.207368</td>\n",
       "      <td>-0.293397</td>\n",
       "      <td>-0.350279</td>\n",
       "      <td>-0.397557</td>\n",
       "      <td>-0.389958</td>\n",
       "      <td>-0.354707</td>\n",
       "      <td>-0.304346</td>\n",
       "      <td>-0.269706</td>\n",
       "      <td>-0.230373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077477</td>\n",
       "      <td>0.114251</td>\n",
       "      <td>0.084068</td>\n",
       "      <td>0.111139</td>\n",
       "      <td>0.123935</td>\n",
       "      <td>0.100647</td>\n",
       "      <td>0.085524</td>\n",
       "      <td>0.073658</td>\n",
       "      <td>0.079488</td>\n",
       "      <td>-0.018803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>-0.131746</td>\n",
       "      <td>-0.244280</td>\n",
       "      <td>-0.314516</td>\n",
       "      <td>-0.374326</td>\n",
       "      <td>-0.396897</td>\n",
       "      <td>-0.374345</td>\n",
       "      <td>-0.330108</td>\n",
       "      <td>-0.281328</td>\n",
       "      <td>-0.228562</td>\n",
       "      <td>-0.187110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115410</td>\n",
       "      <td>0.102452</td>\n",
       "      <td>0.132755</td>\n",
       "      <td>0.160011</td>\n",
       "      <td>0.143198</td>\n",
       "      <td>0.110509</td>\n",
       "      <td>0.080957</td>\n",
       "      <td>0.066911</td>\n",
       "      <td>-0.011033</td>\n",
       "      <td>-0.089221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>-0.152509</td>\n",
       "      <td>-0.240283</td>\n",
       "      <td>-0.310032</td>\n",
       "      <td>-0.348716</td>\n",
       "      <td>-0.362345</td>\n",
       "      <td>-0.332099</td>\n",
       "      <td>-0.295929</td>\n",
       "      <td>-0.230758</td>\n",
       "      <td>-0.180138</td>\n",
       "      <td>-0.119306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126967</td>\n",
       "      <td>0.177333</td>\n",
       "      <td>0.212189</td>\n",
       "      <td>0.207463</td>\n",
       "      <td>0.182614</td>\n",
       "      <td>0.135566</td>\n",
       "      <td>0.103614</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>-0.062737</td>\n",
       "      <td>-0.090131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3487 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Error1    Error2    Error3    Error4    Error5    Error6    Error7  \\\n",
       "0    -0.025152 -0.059169 -0.086922 -0.105664 -0.117361 -0.111668 -0.072587   \n",
       "1    -0.063227 -0.100190 -0.124349 -0.133111 -0.135924 -0.093210 -0.005019   \n",
       "2    -0.065101 -0.098766 -0.120196 -0.128050 -0.095446 -0.015322  0.113070   \n",
       "3    -0.039715 -0.067955 -0.089993 -0.070940 -0.005447  0.104153  0.219329   \n",
       "4    -0.014958 -0.030828 -0.018811  0.037041  0.127185  0.218611  0.237795   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3482  0.054111 -0.032766 -0.126196 -0.209254 -0.295928 -0.338872 -0.351584   \n",
       "3483 -0.037913 -0.144251 -0.224120 -0.300407 -0.353838 -0.379406 -0.359524   \n",
       "3484 -0.102279 -0.207368 -0.293397 -0.350279 -0.397557 -0.389958 -0.354707   \n",
       "3485 -0.131746 -0.244280 -0.314516 -0.374326 -0.396897 -0.374345 -0.330108   \n",
       "3486 -0.152509 -0.240283 -0.310032 -0.348716 -0.362345 -0.332099 -0.295929   \n",
       "\n",
       "        Error8    Error9   Error10  ...   Error15   Error16   Error17  \\\n",
       "0    -0.002126  0.105017  0.195354  ...  0.036005  0.026349  0.068125   \n",
       "1     0.110245  0.214102  0.226699  ...  0.047167  0.086932  0.096857   \n",
       "2     0.224445  0.240981  0.127046  ...  0.101161  0.108593  0.157137   \n",
       "3     0.241580  0.123726  0.065646  ...  0.103100  0.155179  0.202517   \n",
       "4     0.123505  0.062009  0.073066  ...  0.143826  0.198024  0.219623   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3482 -0.334105 -0.304751 -0.271728  ... -0.048482 -0.008425  0.120448   \n",
       "3483 -0.321779 -0.284710 -0.260562  ... -0.031617  0.102229  0.126131   \n",
       "3484 -0.304346 -0.269706 -0.230373  ...  0.077477  0.114251  0.084068   \n",
       "3485 -0.281328 -0.228562 -0.187110  ...  0.115410  0.102452  0.132755   \n",
       "3486 -0.230758 -0.180138 -0.119306  ...  0.126967  0.177333  0.212189   \n",
       "\n",
       "       Error18   Error19   Error20   Error21   Error22   Error23   Error24  \n",
       "0     0.067560  0.100296  0.109592  0.109791  0.098673  0.067444  0.031502  \n",
       "1     0.141688  0.169579  0.153286  0.131086  0.080365  0.060904  0.022698  \n",
       "2     0.199763  0.197983  0.169782  0.117008  0.083719  0.072227  0.009551  \n",
       "3     0.221129  0.207567  0.155997  0.119076  0.093642  0.059731  0.024925  \n",
       "4     0.222350  0.183849  0.150177  0.112134  0.055202  0.035915  0.029576  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3482  0.145196  0.116243  0.134227  0.160606  0.174015  0.179981  0.155651  \n",
       "3483  0.096329  0.116942  0.130462  0.126939  0.132640  0.133849  0.112907  \n",
       "3484  0.111139  0.123935  0.100647  0.085524  0.073658  0.079488 -0.018803  \n",
       "3485  0.160011  0.143198  0.110509  0.080957  0.066911 -0.011033 -0.089221  \n",
       "3486  0.207463  0.182614  0.135566  0.103614  0.001180 -0.062737 -0.090131  \n",
       "\n",
       "[3487 rows x 24 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teY = testY.reshape(-1,24)\n",
    "e_te = testPredict-teY\n",
    "er_df = pd.DataFrame(e_te, columns=Erindex)\n",
    "er_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e579d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3487, 24)\n"
     ]
    }
   ],
   "source": [
    "prnorm = np.array(pr_df)\n",
    "ernorm =np.array(er_df)\n",
    "print(ernorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18e3adba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50811783,  0.4704656 ,  0.48071399,  0.48790419,  0.49578348,\n",
       "         0.4992941 ,  0.50634456,  0.51036179,  0.51579267,  0.52419811,\n",
       "         0.51140893,  0.50170243,  0.49157917,  0.47797626,  0.47038782,\n",
       "         0.46269685,  0.45493549,  0.44939682,  0.44489545,  0.44410384,\n",
       "         0.43786734,  0.43337652,  0.44088262,  0.45506158,  0.4613336 ,\n",
       "        -0.03765223, -0.07962751, -0.10928673, -0.06183877, -0.00976501,\n",
       "         0.03207798,  0.07353729,  0.11525978,  0.16584857,  0.19283723,\n",
       "         0.22001501,  0.25538701,  0.2756005 ,  0.29764498,  0.31278879,\n",
       "         0.34633922,  0.31510706,  0.31953042,  0.35984143,  0.37783421,\n",
       "         0.38429013,  0.38726414,  0.38770737,  0.36072079],\n",
       "       [ 0.56034151,  0.52175081,  0.53601712,  0.55225676,  0.56086314,\n",
       "         0.55998719,  0.5562647 ,  0.54811174,  0.54038852,  0.53816265,\n",
       "         0.51849908,  0.50560772,  0.49681836,  0.48172596,  0.47437435,\n",
       "         0.46129227,  0.45501441,  0.44854352,  0.44340152,  0.44408879,\n",
       "         0.43740666,  0.4328222 ,  0.4398196 ,  0.44856736,  0.45086822,\n",
       "        -0.0385907 , -0.0611738 , -0.00536549,  0.05180402,  0.08572061,\n",
       "         0.1194402 ,  0.14757885,  0.18203898,  0.21959095,  0.23681166,\n",
       "         0.26941556,  0.2944426 ,  0.30898312,  0.3244663 ,  0.35269599,\n",
       "         0.32072465,  0.3231785 ,  0.3591391 ,  0.38405565,  0.38832027,\n",
       "         0.37920372,  0.37246539,  0.34795455,  0.29973998]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etedat = np.concatenate((npnorm22[:prnorm.shape[0],:], prnorm, ernorm), axis=1)\n",
    "etedat[169:171,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6ae3445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3317, 1, 49)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_timesteps = 1\n",
    "eteX, eteY = create_dataset(etedat, timesteps, output_timesteps)\n",
    "eteY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a13b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eteY = eteY[:,:,-24:].reshape(-1,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e6586f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31213, 1, 49)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df2 = np.array(norm_df2)\n",
    "output_timesteps = 1\n",
    "Xe, Ye = create_dataset(norm_df2, timesteps, output_timesteps)\n",
    "Ye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4138b952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31213, 168, 49)\n",
      "(31213, 24)\n"
     ]
    }
   ],
   "source": [
    "Ye = Ye[:,:,-24:].reshape(-1,24)\n",
    "print(Xe.shape)\n",
    "print(Ye.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d006aae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4195"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73cd92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(Xe.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "Xe = Xe[idx]\n",
    "Ye = Ye[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99051f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3317, 168, 49), (3317, 24), (31213, 168, 49), (31213, 24))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eteX.shape, eteY.shape, Xe.shape, Ye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "235b2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_mae(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))*10+K.mean(K.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57e6fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 168, 49)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 49, 168)      0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 49, 168)      28392       permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 49, 168)      28392       permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_38 (Multiply)          (None, 49, 168)      0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 168, 49)      0           multiply_38[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_39 (Multiply)          (None, 168, 49)      0           input_4[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 168, 256)     12800       multiply_39[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 168, 256)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 168, 256)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_40 (Multiply)          (None, 168, 256)     0           activation_64[0][0]              \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 168, 49)      25137       multiply_40[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 168, 49)      0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 168, 49)      0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_41 (Multiply)          (None, 168, 49)      0           activation_66[0][0]              \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 168, 49)      0           input_4[0][0]                    \n",
      "                                                                 multiply_41[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_8 (Subtract)           (None, 168, 49)      0           input_4[0][0]                    \n",
      "                                                                 multiply_41[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 168, 256)     12800       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 168, 256)     12800       subtract_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 168, 256)     0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 168, 256)     0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 168, 256)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 168, 256)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_42 (Multiply)          (None, 168, 256)     0           activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_44 (Multiply)          (None, 168, 256)     0           activation_72[0][0]              \n",
      "                                                                 activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 168, 49)      25137       multiply_42[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 168, 49)      25137       multiply_44[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 168, 49)      0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 168, 49)      0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 168, 49)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 168, 49)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_43 (Multiply)          (None, 168, 49)      0           activation_70[0][0]              \n",
      "                                                                 activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_45 (Multiply)          (None, 168, 49)      0           activation_74[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 168, 49)      0           add_10[0][0]                     \n",
      "                                                                 multiply_43[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_9 (Subtract)           (None, 168, 49)      0           subtract_8[0][0]                 \n",
      "                                                                 multiply_45[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 168, 196)     0           add_11[0][0]                     \n",
      "                                                                 subtract_9[0][0]                 \n",
      "                                                                 add_10[0][0]                     \n",
      "                                                                 subtract_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 168, 256)     50432       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 168, 256)     50432       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 168, 256)     0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 168, 256)     0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 168, 256)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 168, 256)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_46 (Multiply)          (None, 168, 256)     0           activation_76[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_48 (Multiply)          (None, 168, 256)     0           activation_80[0][0]              \n",
      "                                                                 activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 168, 49)      25137       multiply_46[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 168, 49)      25137       multiply_48[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 168, 49)      0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 168, 49)      0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 168, 49)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 168, 49)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_47 (Multiply)          (None, 168, 49)      0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_49 (Multiply)          (None, 168, 49)      0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 168, 49)      0           add_11[0][0]                     \n",
      "                                                                 multiply_47[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_10 (Subtract)          (None, 168, 49)      0           subtract_9[0][0]                 \n",
      "                                                                 multiply_49[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 168, 294)     0           add_12[0][0]                     \n",
      "                                                                 subtract_10[0][0]                \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 168, 256)     75520       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 168, 256)     75520       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 168, 256)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 168, 256)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 168, 256)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 168, 256)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_50 (Multiply)          (None, 168, 256)     0           activation_84[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_52 (Multiply)          (None, 168, 256)     0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 168, 49)      25137       multiply_50[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 168, 49)      25137       multiply_52[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 168, 49)      0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 168, 49)      0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 168, 49)      0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 168, 49)      0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_51 (Multiply)          (None, 168, 49)      0           activation_86[0][0]              \n",
      "                                                                 activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_53 (Multiply)          (None, 168, 49)      0           activation_90[0][0]              \n",
      "                                                                 activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 168, 49)      0           add_12[0][0]                     \n",
      "                                                                 multiply_51[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_11 (Subtract)          (None, 168, 49)      0           subtract_9[0][0]                 \n",
      "                                                                 multiply_53[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 168, 392)     0           add_13[0][0]                     \n",
      "                                                                 subtract_11[0][0]                \n",
      "                                                                 concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 168, 256)     100608      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 168, 256)     100608      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 168, 256)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 168, 256)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 168, 256)     0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 168, 256)     0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_54 (Multiply)          (None, 168, 256)     0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_56 (Multiply)          (None, 168, 256)     0           activation_96[0][0]              \n",
      "                                                                 activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 168, 49)      25137       multiply_54[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 168, 49)      25137       multiply_56[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 168, 49)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 168, 49)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 168, 49)      0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 168, 49)      0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_55 (Multiply)          (None, 168, 49)      0           activation_94[0][0]              \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_57 (Multiply)          (None, 168, 49)      0           activation_98[0][0]              \n",
      "                                                                 activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 168, 49)      0           add_13[0][0]                     \n",
      "                                                                 multiply_55[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_12 (Subtract)          (None, 168, 49)      0           subtract_11[0][0]                \n",
      "                                                                 multiply_57[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 168, 490)     0           add_14[0][0]                     \n",
      "                                                                 subtract_12[0][0]                \n",
      "                                                                 concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 168, 98)      0           add_14[0][0]                     \n",
      "                                                                 subtract_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 168, 1470)    0           concatenate_7[0][0]              \n",
      "                                                                 concatenate_8[0][0]              \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 concatenate_10[0][0]             \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 168, 720)     1180080     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 168, 720)     0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 168, 360)     320040      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 168, 360)     0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 360)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 24)           8664        global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 2,283,321\n",
      "Trainable params: 2,283,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_features = Xe.shape[2]\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    alpha=1.0\n",
    "    gamma=1.2\n",
    "    beta = 1\n",
    "    hfilters = 256\n",
    "    hkernel_size1 = 1\n",
    "    hkernel_size2 = 2\n",
    "    visible1e = Input(shape=(timesteps, num_features))\n",
    "   \n",
    "    per1e = Permute((2,1))(visible1e)\n",
    "    den1ae = Dense(timesteps, activation='tanh')(per1e)\n",
    "    den1be = Dense(timesteps, activation='sigmoid')(per1e)\n",
    "    den1e = Multiply()([den1ae, den1be])\n",
    "    per2e = Permute((2,1), name='attention_vec')(den1e)\n",
    "    mul1e = Multiply()([visible1e, per2e])\n",
    "    \n",
    "    d1e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=1)(mul1e)\n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    d1e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d1e)\n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    res01ae = Add()([visible1e, d1e])   # (100, 25) (100, 25)\n",
    "    res01be = Subtract()([visible1e, d1e])\n",
    "\n",
    "    d1e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res01ae)\n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    d1e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d1e)    \n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    \n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    res02ae = Add()([res01ae, d1e])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res01be) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    d2e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d2e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    res02be = Subtract()([res01be, d2e])   # (100, 25) (100, 25) \n",
    "    res02e = Concatenate()([res02ae, res02be, res01ae, res01be])\n",
    "    \n",
    "    d1e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res02e)\n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    d1e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=8)(d1e)    \n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    res03ae = Add()([res02ae, d1e])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res02e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    d2e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=8)(d2e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    res03be = Subtract()([res02be, d2e])   # (100, 25) (100, 25)\n",
    "    res03e = Concatenate()([res03ae, res03be, res02e])\n",
    "    \n",
    "    d1e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=8)(res03e)\n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    d1e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d1e)    \n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    res04ae = Add()([res03ae, d1e])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=8)(res03e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    d2e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d2e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    res04be = Subtract()([res02be, d2e])   # (100, 25) (100, 25)\n",
    "    res04e = Concatenate()([res04ae, res04be, res03e])\n",
    "    \n",
    "    d1e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res04e)\n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    d1e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d1e)    \n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    res05ae = Add()([res04ae, d1e])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res04e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    d2e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d2e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    \n",
    "    res05be = Subtract()([res04be, d2e])   # (100, 25) (100, 25)\n",
    "    res05e = Concatenate()([res05ae, res05be, res04e])\n",
    "    \n",
    "    d1e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res05e)\n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "    \n",
    "    d1e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=1)(d1e)    \n",
    "    d1ae = Activation(activations.tanh)(d1e)\n",
    "    d1be = Activation(activations.sigmoid)(d1e)\n",
    "    d1e = Multiply()([d1ae, d1be])\n",
    "\n",
    "    res06ae = Add()([res05ae, d1e])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2e = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res05e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "    d2e = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=1)(d2e) \n",
    "    d2ae = Activation(activations.tanh)(d2e)\n",
    "    d2be = Activation(activations.sigmoid)(d2e)\n",
    "    d2e = Multiply()([d2ae, d2be])\n",
    "\n",
    "    res06be = Subtract()([res05be, d2e])   # (100, 25) (100, 25)\n",
    "    res06e = Concatenate()([res05ae, res05be])\n",
    "    \n",
    "    res10e = Concatenate()([res02e, res03e, res04e, res05e, res06e])   # \n",
    "    \n",
    "    #print('res10 :', res10.shape)  # (None, 24, 11) \n",
    "    \n",
    "    oute = Conv1D(720, 1, padding='same', activation=PReLU())(res10e)   # 256, 11X10=110\n",
    "    oute = Dropout(0.2)(oute)   #SpatialDropout1D\n",
    "    \n",
    "    oute = Conv1D(360, 1, padding='same', activation=PReLU())(oute) # 512,  110X5=550\n",
    "    oute = Dropout(0.2)(oute)\n",
    "    \n",
    "    oute = GlobalAveragePooling1D()(oute) # pool_size=2, strides=1\n",
    "    \n",
    "    oute = Dense(24)(oute) \n",
    "    modele = Model(inputs=[visible1e], outputs=[oute])\n",
    "    \n",
    "    print(modele.summary())\n",
    "    \n",
    "    modele.compile(loss=mse_mae, optimizer='adam', metrics=['mse','mae','mape'])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=50)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "\n",
    "    history_e = LossHistory()\n",
    "    history_e.init()\n",
    "    \n",
    "    #hist = model.fit(trX, trY, epochs=epochs, batch_size=batch_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fb6a70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "211b7fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  6/186 [..............................] - ETA: 23s - loss: 1.7976 - mse: 0.1543 - mae: 0.2542 - mape: 19216.5781WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0586s vs `on_train_batch_end` time: 0.0609s). Check your callbacks.\n",
      "186/186 [==============================] - 31s 147ms/step - loss: 0.1422 - mse: 0.0090 - mae: 0.0517 - mape: 1072.9917 - val_loss: 0.1486 - val_mse: 0.0080 - val_mae: 0.0689 - val_mape: 366.2673\n",
      "Epoch 2/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0452 - mse: 0.0016 - mae: 0.0295 - mape: 476.7033 - val_loss: 0.0845 - val_mse: 0.0038 - val_mae: 0.0462 - val_mape: 198.3948\n",
      "Epoch 3/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0328 - mse: 9.5047e-04 - mae: 0.0233 - mape: 415.8075 - val_loss: 0.0666 - val_mse: 0.0028 - val_mae: 0.0391 - val_mape: 195.7670\n",
      "Epoch 4/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0272 - mse: 7.0617e-04 - mae: 0.0201 - mape: 325.4146 - val_loss: 0.0543 - val_mse: 0.0021 - val_mae: 0.0334 - val_mape: 234.8499\n",
      "Epoch 5/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0240 - mse: 5.8029e-04 - mae: 0.0182 - mape: 297.9801 - val_loss: 0.0542 - val_mse: 0.0022 - val_mae: 0.0326 - val_mape: 194.9598\n",
      "Epoch 6/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0220 - mse: 5.0698e-04 - mae: 0.0170 - mape: 257.6234 - val_loss: 0.0525 - val_mse: 0.0021 - val_mae: 0.0317 - val_mape: 192.6459\n",
      "Epoch 7/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0204 - mse: 4.5008e-04 - mae: 0.0159 - mape: 263.7137 - val_loss: 0.0464 - val_mse: 0.0017 - val_mae: 0.0289 - val_mape: 171.2506\n",
      "Epoch 8/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0195 - mse: 4.1766e-04 - mae: 0.0153 - mape: 252.4366 - val_loss: 0.0422 - val_mse: 0.0015 - val_mae: 0.0272 - val_mape: 136.8531\n",
      "Epoch 9/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0186 - mse: 3.8819e-04 - mae: 0.0147 - mape: 235.4690 - val_loss: 0.0420 - val_mse: 0.0015 - val_mae: 0.0272 - val_mape: 134.6359\n",
      "Epoch 10/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0180 - mse: 3.6688e-04 - mae: 0.0143 - mape: 269.4635 - val_loss: 0.0363 - val_mse: 0.0012 - val_mae: 0.0247 - val_mape: 140.6512\n",
      "Epoch 11/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0174 - mse: 3.4991e-04 - mae: 0.0139 - mape: 218.2946 - val_loss: 0.0356 - val_mse: 0.0011 - val_mae: 0.0244 - val_mape: 126.4130\n",
      "Epoch 12/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0170 - mse: 3.3731e-04 - mae: 0.0137 - mape: 207.8983 - val_loss: 0.0341 - val_mse: 0.0011 - val_mae: 0.0235 - val_mape: 154.3467\n",
      "Epoch 13/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0167 - mse: 3.2733e-04 - mae: 0.0134 - mape: 236.6081 - val_loss: 0.0344 - val_mse: 0.0011 - val_mae: 0.0237 - val_mape: 127.8475\n",
      "Epoch 14/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0163 - mse: 3.1476e-04 - mae: 0.0132 - mape: 203.7664 - val_loss: 0.0326 - val_mse: 9.7336e-04 - val_mae: 0.0229 - val_mape: 115.0751\n",
      "Epoch 15/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0159 - mse: 3.0400e-04 - mae: 0.0129 - mape: 221.3977 - val_loss: 0.0301 - val_mse: 8.5869e-04 - val_mae: 0.0216 - val_mape: 106.6621\n",
      "Epoch 16/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0158 - mse: 2.9903e-04 - mae: 0.0128 - mape: 197.2028 - val_loss: 0.0304 - val_mse: 8.7470e-04 - val_mae: 0.0216 - val_mape: 126.0060\n",
      "Epoch 17/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0154 - mse: 2.8720e-04 - mae: 0.0125 - mape: 197.9103 - val_loss: 0.0298 - val_mse: 8.4480e-04 - val_mae: 0.0214 - val_mape: 111.4512\n",
      "Epoch 18/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0152 - mse: 2.8065e-04 - mae: 0.0124 - mape: 207.1190 - val_loss: 0.0293 - val_mse: 8.1499e-04 - val_mae: 0.0211 - val_mape: 113.6289\n",
      "Epoch 19/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0150 - mse: 2.7705e-04 - mae: 0.0123 - mape: 203.3283 - val_loss: 0.0290 - val_mse: 8.0782e-04 - val_mae: 0.0210 - val_mape: 119.5014\n",
      "Epoch 20/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0147 - mse: 2.6852e-04 - mae: 0.0120 - mape: 187.6247 - val_loss: 0.0294 - val_mse: 8.1961e-04 - val_mae: 0.0212 - val_mape: 118.4063\n",
      "Epoch 21/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0146 - mse: 2.6574e-04 - mae: 0.0120 - mape: 207.7367 - val_loss: 0.0301 - val_mse: 8.4342e-04 - val_mae: 0.0216 - val_mape: 119.4277\n",
      "Epoch 22/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0144 - mse: 2.6036e-04 - mae: 0.0118 - mape: 193.2520 - val_loss: 0.0291 - val_mse: 8.0883e-04 - val_mae: 0.0211 - val_mape: 118.8855\n",
      "Epoch 23/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0143 - mse: 2.5710e-04 - mae: 0.0118 - mape: 180.3208 - val_loss: 0.0299 - val_mse: 8.4305e-04 - val_mae: 0.0215 - val_mape: 141.9117\n",
      "Epoch 24/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0142 - mse: 2.5306e-04 - mae: 0.0116 - mape: 191.7745 - val_loss: 0.0293 - val_mse: 8.1108e-04 - val_mae: 0.0212 - val_mape: 115.2295\n",
      "Epoch 25/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0140 - mse: 2.4784e-04 - mae: 0.0115 - mape: 195.9478 - val_loss: 0.0279 - val_mse: 7.5427e-04 - val_mae: 0.0204 - val_mape: 109.8645\n",
      "Epoch 26/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0139 - mse: 2.4576e-04 - mae: 0.0115 - mape: 179.2509 - val_loss: 0.0285 - val_mse: 7.8048e-04 - val_mae: 0.0207 - val_mape: 112.9411\n",
      "Epoch 27/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0138 - mse: 2.4225e-04 - mae: 0.0114 - mape: 191.6768 - val_loss: 0.0276 - val_mse: 7.4387e-04 - val_mae: 0.0202 - val_mape: 142.3989\n",
      "Epoch 28/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0136 - mse: 2.3828e-04 - mae: 0.0113 - mape: 196.0356 - val_loss: 0.0280 - val_mse: 7.5772e-04 - val_mae: 0.0204 - val_mape: 121.8079\n",
      "Epoch 29/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0135 - mse: 2.3541e-04 - mae: 0.0112 - mape: 190.3512 - val_loss: 0.0274 - val_mse: 7.3358e-04 - val_mae: 0.0201 - val_mape: 115.7717\n",
      "Epoch 30/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0134 - mse: 2.3054e-04 - mae: 0.0110 - mape: 208.7566 - val_loss: 0.0270 - val_mse: 7.1450e-04 - val_mae: 0.0199 - val_mape: 125.6113\n",
      "Epoch 31/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0132 - mse: 2.2667e-04 - mae: 0.0109 - mape: 185.4872 - val_loss: 0.0280 - val_mse: 7.5839e-04 - val_mae: 0.0204 - val_mape: 116.7486\n",
      "Epoch 32/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0131 - mse: 2.2329e-04 - mae: 0.0108 - mape: 179.6457 - val_loss: 0.0283 - val_mse: 7.7558e-04 - val_mae: 0.0205 - val_mape: 118.6774\n",
      "Epoch 33/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0130 - mse: 2.2054e-04 - mae: 0.0108 - mape: 176.2048 - val_loss: 0.0285 - val_mse: 7.7796e-04 - val_mae: 0.0207 - val_mape: 123.4585\n",
      "Epoch 34/1000\n",
      "186/186 [==============================] - 26s 137ms/step - loss: 0.0129 - mse: 2.1825e-04 - mae: 0.0107 - mape: 164.9093 - val_loss: 0.0275 - val_mse: 7.3575e-04 - val_mae: 0.0202 - val_mape: 118.9877\n",
      "Epoch 35/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0128 - mse: 2.1579e-04 - mae: 0.0106 - mape: 171.6349 - val_loss: 0.0280 - val_mse: 7.5962e-04 - val_mae: 0.0204 - val_mape: 126.7451\n",
      "Epoch 36/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0127 - mse: 2.1295e-04 - mae: 0.0105 - mape: 215.0572 - val_loss: 0.0285 - val_mse: 7.7634e-04 - val_mae: 0.0208 - val_mape: 141.6737\n",
      "Epoch 37/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0126 - mse: 2.1121e-04 - mae: 0.0105 - mape: 171.2424 - val_loss: 0.0279 - val_mse: 7.5227e-04 - val_mae: 0.0203 - val_mape: 117.7121\n",
      "Epoch 38/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0125 - mse: 2.0881e-04 - mae: 0.0105 - mape: 175.3774 - val_loss: 0.0282 - val_mse: 7.6835e-04 - val_mae: 0.0205 - val_mape: 103.7129\n",
      "Epoch 39/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0124 - mse: 2.0518e-04 - mae: 0.0103 - mape: 176.5890 - val_loss: 0.0280 - val_mse: 7.6494e-04 - val_mae: 0.0204 - val_mape: 110.1111\n",
      "Epoch 40/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0124 - mse: 2.0664e-04 - mae: 0.0104 - mape: 190.9720 - val_loss: 0.0298 - val_mse: 8.4268e-04 - val_mae: 0.0214 - val_mape: 123.7372\n",
      "Epoch 41/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0123 - mse: 2.0362e-04 - mae: 0.0103 - mape: 175.1024 - val_loss: 0.0305 - val_mse: 8.6880e-04 - val_mae: 0.0218 - val_mape: 138.9697\n",
      "Epoch 42/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0123 - mse: 2.0270e-04 - mae: 0.0103 - mape: 175.1039 - val_loss: 0.0316 - val_mse: 9.2075e-04 - val_mae: 0.0223 - val_mape: 134.7759\n",
      "Epoch 43/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0122 - mse: 2.0111e-04 - mae: 0.0102 - mape: 189.1910 - val_loss: 0.0300 - val_mse: 8.5171e-04 - val_mae: 0.0215 - val_mape: 110.4071\n",
      "Epoch 44/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0122 - mse: 1.9966e-04 - mae: 0.0102 - mape: 183.6644 - val_loss: 0.0312 - val_mse: 8.9790e-04 - val_mae: 0.0222 - val_mape: 124.7036\n",
      "Epoch 45/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0121 - mse: 1.9782e-04 - mae: 0.0101 - mape: 168.1460 - val_loss: 0.0313 - val_mse: 9.0030e-04 - val_mae: 0.0223 - val_mape: 137.5455\n",
      "Epoch 46/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0121 - mse: 1.9742e-04 - mae: 0.0101 - mape: 162.5600 - val_loss: 0.0338 - val_mse: 0.0010 - val_mae: 0.0236 - val_mape: 147.5046\n",
      "Epoch 47/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0121 - mse: 1.9788e-04 - mae: 0.0102 - mape: 162.0875 - val_loss: 0.0352 - val_mse: 0.0011 - val_mae: 0.0244 - val_mape: 122.5547\n",
      "Epoch 48/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0120 - mse: 1.9372e-04 - mae: 0.0101 - mape: 183.8640 - val_loss: 0.0379 - val_mse: 0.0012 - val_mae: 0.0258 - val_mape: 129.4632\n",
      "Epoch 49/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0120 - mse: 1.9259e-04 - mae: 0.0100 - mape: 174.9908 - val_loss: 0.0342 - val_mse: 0.0011 - val_mae: 0.0237 - val_mape: 142.2820\n",
      "Epoch 50/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0119 - mse: 1.9091e-04 - mae: 0.0100 - mape: 188.1983 - val_loss: 0.0359 - val_mse: 0.0011 - val_mae: 0.0246 - val_mape: 128.5806\n",
      "Epoch 51/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0117 - mse: 1.8712e-04 - mae: 0.0098 - mape: 184.2253 - val_loss: 0.0343 - val_mse: 0.0011 - val_mae: 0.0238 - val_mape: 133.9545\n",
      "Epoch 52/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0116 - mse: 1.8447e-04 - mae: 0.0098 - mape: 169.0962 - val_loss: 0.0345 - val_mse: 0.0010 - val_mae: 0.0240 - val_mape: 148.9711\n",
      "Epoch 53/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0116 - mse: 1.8337e-04 - mae: 0.0097 - mape: 172.1237 - val_loss: 0.0376 - val_mse: 0.0012 - val_mae: 0.0256 - val_mape: 157.6745\n",
      "Epoch 54/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0115 - mse: 1.8149e-04 - mae: 0.0097 - mape: 194.2481 - val_loss: 0.0369 - val_mse: 0.0012 - val_mae: 0.0251 - val_mape: 138.7895\n",
      "Epoch 55/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0115 - mse: 1.8069e-04 - mae: 0.0097 - mape: 148.0506 - val_loss: 0.0361 - val_mse: 0.0011 - val_mae: 0.0248 - val_mape: 128.0517\n",
      "Epoch 56/1000\n",
      "186/186 [==============================] - 26s 139ms/step - loss: 0.0114 - mse: 1.7892e-04 - mae: 0.0096 - mape: 169.7928 - val_loss: 0.0405 - val_mse: 0.0013 - val_mae: 0.0272 - val_mape: 137.9182\n",
      "Epoch 57/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0114 - mse: 1.7715e-04 - mae: 0.0096 - mape: 167.4546 - val_loss: 0.0375 - val_mse: 0.0012 - val_mae: 0.0256 - val_mape: 128.8255\n",
      "Epoch 58/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0114 - mse: 1.7723e-04 - mae: 0.0096 - mape: 160.4648 - val_loss: 0.0398 - val_mse: 0.0013 - val_mae: 0.0268 - val_mape: 146.0004\n",
      "Epoch 59/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0113 - mse: 1.7446e-04 - mae: 0.0095 - mape: 163.5568 - val_loss: 0.0397 - val_mse: 0.0013 - val_mae: 0.0266 - val_mape: 163.7266\n",
      "Epoch 60/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0113 - mse: 1.7436e-04 - mae: 0.0095 - mape: 164.5971 - val_loss: 0.0413 - val_mse: 0.0014 - val_mae: 0.0276 - val_mape: 155.1378\n",
      "Epoch 61/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0113 - mse: 1.7455e-04 - mae: 0.0095 - mape: 168.9459 - val_loss: 0.0377 - val_mse: 0.0012 - val_mae: 0.0256 - val_mape: 127.4073\n",
      "Epoch 62/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0114 - mse: 1.7625e-04 - mae: 0.0096 - mape: 165.4082 - val_loss: 0.0385 - val_mse: 0.0012 - val_mae: 0.0261 - val_mape: 130.5127\n",
      "Epoch 63/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0114 - mse: 1.7798e-04 - mae: 0.0097 - mape: 173.6083 - val_loss: 0.0399 - val_mse: 0.0013 - val_mae: 0.0269 - val_mape: 134.7874\n",
      "Epoch 64/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0113 - mse: 1.7407e-04 - mae: 0.0095 - mape: 179.0079 - val_loss: 0.0404 - val_mse: 0.0013 - val_mae: 0.0272 - val_mape: 141.4821\n",
      "Epoch 65/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0111 - mse: 1.7065e-04 - mae: 0.0094 - mape: 181.1535 - val_loss: 0.0417 - val_mse: 0.0014 - val_mae: 0.0278 - val_mape: 157.2339\n",
      "Epoch 66/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0110 - mse: 1.6775e-04 - mae: 0.0093 - mape: 163.1582 - val_loss: 0.0387 - val_mse: 0.0013 - val_mae: 0.0262 - val_mape: 169.2598\n",
      "Epoch 67/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0111 - mse: 1.6844e-04 - mae: 0.0094 - mape: 173.4100 - val_loss: 0.0448 - val_mse: 0.0016 - val_mae: 0.0292 - val_mape: 177.2432\n",
      "Epoch 68/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0109 - mse: 1.6481e-04 - mae: 0.0093 - mape: 180.4377 - val_loss: 0.0425 - val_mse: 0.0014 - val_mae: 0.0282 - val_mape: 159.1331\n",
      "Epoch 69/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0108 - mse: 1.6227e-04 - mae: 0.0092 - mape: 158.8171 - val_loss: 0.0410 - val_mse: 0.0014 - val_mae: 0.0275 - val_mape: 139.9626\n",
      "Epoch 70/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0108 - mse: 1.6265e-04 - mae: 0.0092 - mape: 173.6496 - val_loss: 0.0434 - val_mse: 0.0015 - val_mae: 0.0286 - val_mape: 152.8101\n",
      "Epoch 71/1000\n",
      "186/186 [==============================] - 25s 137ms/step - loss: 0.0109 - mse: 1.6258e-04 - mae: 0.0092 - mape: 159.3197 - val_loss: 0.0427 - val_mse: 0.0014 - val_mae: 0.0284 - val_mape: 145.9799\n",
      "Epoch 72/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0110 - mse: 1.6548e-04 - mae: 0.0093 - mape: 157.8703 - val_loss: 0.0522 - val_mse: 0.0019 - val_mae: 0.0331 - val_mape: 185.3564\n",
      "Epoch 73/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0109 - mse: 1.6463e-04 - mae: 0.0093 - mape: 182.8996 - val_loss: 0.0503 - val_mse: 0.0018 - val_mae: 0.0322 - val_mape: 188.9595\n",
      "Epoch 74/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0107 - mse: 1.5931e-04 - mae: 0.0092 - mape: 152.2778 - val_loss: 0.0441 - val_mse: 0.0015 - val_mae: 0.0290 - val_mape: 141.3501\n",
      "Epoch 75/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0106 - mse: 1.5566e-04 - mae: 0.0090 - mape: 160.9012 - val_loss: 0.0465 - val_mse: 0.0016 - val_mae: 0.0303 - val_mape: 149.8053\n",
      "Epoch 76/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0105 - mse: 1.5348e-04 - mae: 0.0089 - mape: 153.6815 - val_loss: 0.0478 - val_mse: 0.0017 - val_mae: 0.0310 - val_mape: 172.8776\n",
      "Epoch 77/1000\n",
      "186/186 [==============================] - 26s 139ms/step - loss: 0.0104 - mse: 1.5176e-04 - mae: 0.0089 - mape: 156.3868 - val_loss: 0.0478 - val_mse: 0.0017 - val_mae: 0.0307 - val_mape: 166.2139\n",
      "Epoch 78/1000\n",
      "186/186 [==============================] - 26s 138ms/step - loss: 0.0103 - mse: 1.4998e-04 - mae: 0.0088 - mape: 151.9544 - val_loss: 0.0495 - val_mse: 0.0018 - val_mae: 0.0317 - val_mape: 189.1922\n",
      "Epoch 79/1000\n",
      "186/186 [==============================] - 25s 136ms/step - loss: 0.0104 - mse: 1.5076e-04 - mae: 0.0089 - mape: 146.4332 - val_loss: 0.0485 - val_mse: 0.0017 - val_mae: 0.0314 - val_mape: 198.3260\n",
      "Epoch 80/1000\n",
      "186/186 [==============================] - 25s 135ms/step - loss: 0.0103 - mse: 1.4843e-04 - mae: 0.0088 - mape: 156.9017 - val_loss: 0.0503 - val_mse: 0.0018 - val_mae: 0.0320 - val_mape: 170.1852\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "    histe = modele.fit(Xe, Ye, epochs=epochs, batch_size=b_size, shuffle=False, validation_data=(eteX, eteY), callbacks=[history_e, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ead4a",
   "metadata": {},
   "source": [
    "### Saving FFEL Model Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfee1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "eloss_history = histe.history['loss']\n",
    "valeloss_history = histe.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67d0e78a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('elosshistory.txt',(eloss_history, valeloss_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98e1b4af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4193"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a528e17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "modele.save('Error Learning Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423611ff",
   "metadata": {},
   "source": [
    "## FFEL Model Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c94ddfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 168\n",
    "trainePredict = modele.predict(Xe, batch_size=batch_size)\n",
    "etePredict = modele.predict(eteX, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e82bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Training Score > MSE ==  0.00012109809416136521  MAE ==  0.007840872043127885  RMSE ==  0.011004457922195223\n"
     ]
    }
   ],
   "source": [
    "trePredict = trainePredict.reshape([-1])\n",
    "trainYe = Ye.reshape([-1])\n",
    "\n",
    "print('Error Training Score > MSE == ', (np.mean(np.square(trainYe-trePredict))), ' MAE == ', mean_absolute_error(trainYe,trePredict), ' RMSE == ', np.sqrt(np.mean(np.square(trainYe-trePredict))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6a565d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Training Score > MSE ==  0.0018273311390400672  MAE ==  0.03202747032164778  RMSE ==  0.042747293938214\n"
     ]
    }
   ],
   "source": [
    "etestPredict = etePredict.reshape([-1])\n",
    "testYe = eteY.reshape([-1])\n",
    "\n",
    "print('Error Training Score > MSE == ', (np.mean(np.square(testYe-etestPredict))), ' MAE == ', mean_absolute_error(testYe,etestPredict), ' RMSE == ', np.sqrt(np.mean(np.square(testYe-etestPredict))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2c0ed",
   "metadata": {},
   "source": [
    "## Final Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f043cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3317, 24)\n"
     ]
    }
   ],
   "source": [
    "testPredict = tePredict.reshape(-1,24)\n",
    "addtestPredict = -etePredict + testPredict[timesteps:-2,:]\n",
    "print(addtestPredict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab6b902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.022425848960914766  MAE ==  0.11691428047581572 MAPE ==  63.64561285557148\n",
      "Error Test Score > MSE ==  0.0018273311357062986  MAE ==  0.03202747024719184 MAPE ==  13.158986826378207\n"
     ]
    }
   ],
   "source": [
    "print('Error Test Score > MSE == ', (np.mean(np.square(teY[timesteps:-2,:]-testPredict[timesteps:-2,:]))), ' MAE == ', mean_absolute_error(teY[timesteps:-2,:], testPredict[timesteps:-2,:]), 'MAPE == ', mean_absolute_percentage_error(teY[timesteps:-2,:], testPredict[timesteps:-2,:]))\n",
    "print('Error Test Score > MSE == ', (np.mean(np.square(teY[timesteps:-2,:]-addtestPredict))), ' MAE == ', mean_absolute_error(teY[timesteps:-2,:], addtestPredict), 'MAPE == ', mean_absolute_percentage_error(teY[timesteps:-2,:], addtestPredict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
